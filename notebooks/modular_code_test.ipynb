{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/soutrik-gpuvm/code/Users/Soutrik.Chowdhury/GeneCpGSequencing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../.\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import prepare_data, dna2int, set_seed\n",
    "import torch\n",
    "from src.dataset import CPGDatasetPackPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from src.model import CpGCounterAdvancedPackPadding\n",
    "from src.trainer import (\n",
    "    training_loop_pack_padded,\n",
    "    validation_loop_pack_padded,\n",
    "    train_model_pack_padded,\n",
    ")\n",
    "from src.utils import (\n",
    "    predict_cpgs_from_dna_pack_padded,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    create_test_data\n",
    ")\n",
    "from src.tuner import objective, tune_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -------------------------------------------------------- Hyperparameters --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dna2int)\n",
    "batch_size = 16\n",
    "embedding_dim = 64\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "weight_decay = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "stop_patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --------------------- Prepare data ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 128 16 128\n",
      "2048 512\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "train_x, train_y = prepare_data(\"variable\", 2048)\n",
    "test_x, test_y = prepare_data(\"variable\", 512)\n",
    "# each sequence has a length of 128 and the total number of sequences is 2048 and 512 for train and test respectively\n",
    "print(\n",
    "    min(map(len, train_x)),\n",
    "    max(map(len, train_x)),\n",
    "    min(map(len, test_x)),\n",
    "    max(map(len, test_x)),\n",
    ")\n",
    "print(len(train_x), len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# pytorch standard dataset\n",
    "train_dataset = CPGDatasetPackPadding(train_x, train_y)\n",
    "val_dataset = CPGDatasetPackPadding(test_x, test_y)\n",
    "\n",
    "# each iteration of the dataset will return a list of sequences and a labels\n",
    "x, y = next(iter(train_dataset))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 115]) torch.Size([16]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# standard dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=CPGDatasetPackPadding.collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=CPGDatasetPackPadding.collate_fn,\n",
    ")\n",
    "\n",
    "# each iteration of the dataloader will return a batch of sequences and labels\n",
    "for x_batch, y_batch, lengths in train_dataloader:\n",
    "    print(x_batch.shape, y_batch.shape, lengths.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ------------------- Model Training and Evaluation -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CpGCounterAdvancedPackPadding(\n",
      "  (embedding): Embedding(6, 64, padding_idx=0)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "new_model = CpGCounterAdvancedPackPadding(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 1/100, Train Loss: 8.2485, Train MAE: 2.2632, Train RMSE: 2.8213, Val Loss: 6.0834, Val MAE: 1.9195, Val RMSE: 2.4287\n",
      "New best validation loss: 6.0834 (previous best: inf)\n",
      "Model checkpoint saved at models/best_cpg_model_advanced_packpad.pth\n",
      "Epoch 2/100, Train Loss: 6.3996, Train MAE: 1.9678, Train RMSE: 2.4737, Val Loss: 5.8931, Val MAE: 1.8903, Val RMSE: 2.3775\n",
      "New best validation loss: 5.8931 (previous best: 6.0834)\n",
      "Model checkpoint saved at models/best_cpg_model_advanced_packpad.pth\n",
      "Epoch 3/100, Train Loss: 6.2039, Train MAE: 1.9421, Train RMSE: 2.4372, Val Loss: 5.9874, Val MAE: 1.8995, Val RMSE: 2.3938\n",
      "No improvement, patience left: 9\n",
      "Epoch 4/100, Train Loss: 5.3221, Train MAE: 1.7435, Train RMSE: 2.2303, Val Loss: 4.8355, Val MAE: 1.6738, Val RMSE: 2.1552\n",
      "New best validation loss: 4.8355 (previous best: 5.8931)\n",
      "Model checkpoint saved at models/best_cpg_model_advanced_packpad.pth\n",
      "Epoch 5/100, Train Loss: 1.9347, Train MAE: 0.9590, Train RMSE: 1.2737, Val Loss: 0.9700, Val MAE: 0.6971, Val RMSE: 0.9717\n",
      "New best validation loss: 0.9700 (previous best: 4.8355)\n",
      "Model checkpoint saved at models/best_cpg_model_advanced_packpad.pth\n",
      "Epoch 6/100, Train Loss: 0.6310, Train MAE: 0.5867, Train RMSE: 0.7418, Val Loss: 0.9230, Val MAE: 0.7932, Val RMSE: 0.9534\n",
      "New best validation loss: 0.9230 (previous best: 0.9700)\n",
      "Model checkpoint saved at models/best_cpg_model_advanced_packpad.pth\n",
      "Epoch 7/100, Train Loss: 0.5537, Train MAE: 0.5456, Train RMSE: 0.6799, Val Loss: 1.4474, Val MAE: 1.0518, Val RMSE: 1.1934\n",
      "No improvement, patience left: 9\n",
      "Epoch 8/100, Train Loss: 0.5311, Train MAE: 0.5274, Train RMSE: 0.6595, Val Loss: 0.0565, Val MAE: 0.1854, Val RMSE: 0.2332\n",
      "New best validation loss: 0.0565 (previous best: 0.9230)\n",
      "Model checkpoint saved at models/best_cpg_model_advanced_packpad.pth\n",
      "Epoch 9/100, Train Loss: 0.4936, Train MAE: 0.5070, Train RMSE: 0.6291, Val Loss: 0.2549, Val MAE: 0.3539, Val RMSE: 0.4886\n",
      "No improvement, patience left: 9\n",
      "Epoch 10/100, Train Loss: 0.4845, Train MAE: 0.5034, Train RMSE: 0.6197, Val Loss: 0.3387, Val MAE: 0.5266, Val RMSE: 0.5782\n",
      "No improvement, patience left: 8\n",
      "Epoch 11/100, Train Loss: 0.4638, Train MAE: 0.4976, Train RMSE: 0.6065, Val Loss: 0.1023, Val MAE: 0.2314, Val RMSE: 0.3130\n",
      "No improvement, patience left: 7\n",
      "Epoch 12/100, Train Loss: 0.3812, Train MAE: 0.4515, Train RMSE: 0.5400, Val Loss: 0.0340, Val MAE: 0.1302, Val RMSE: 0.1801\n",
      "New best validation loss: 0.0340 (previous best: 0.0565)\n",
      "Model checkpoint saved at models/best_cpg_model_advanced_packpad.pth\n",
      "Epoch 13/100, Train Loss: 0.3677, Train MAE: 0.4430, Train RMSE: 0.5238, Val Loss: 0.0269, Val MAE: 0.1163, Val RMSE: 0.1599\n",
      "New best validation loss: 0.0269 (previous best: 0.0340)\n",
      "Model checkpoint saved at models/best_cpg_model_advanced_packpad.pth\n",
      "Epoch 14/100, Train Loss: 0.3625, Train MAE: 0.4391, Train RMSE: 0.5189, Val Loss: 0.0326, Val MAE: 0.1246, Val RMSE: 0.1754\n",
      "No improvement, patience left: 9\n",
      "Epoch 15/100, Train Loss: 0.3554, Train MAE: 0.4361, Train RMSE: 0.5116, Val Loss: 0.0631, Val MAE: 0.1809, Val RMSE: 0.2459\n",
      "No improvement, patience left: 8\n",
      "Epoch 16/100, Train Loss: 0.3501, Train MAE: 0.4341, Train RMSE: 0.5068, Val Loss: 0.0307, Val MAE: 0.1217, Val RMSE: 0.1702\n",
      "No improvement, patience left: 7\n",
      "Epoch 17/100, Train Loss: 0.3393, Train MAE: 0.4282, Train RMSE: 0.4972, Val Loss: 0.0542, Val MAE: 0.1792, Val RMSE: 0.2271\n",
      "No improvement, patience left: 6\n",
      "Epoch 18/100, Train Loss: 0.3366, Train MAE: 0.4271, Train RMSE: 0.4942, Val Loss: 0.0495, Val MAE: 0.1702, Val RMSE: 0.2171\n",
      "No improvement, patience left: 5\n",
      "Epoch 19/100, Train Loss: 0.3355, Train MAE: 0.4272, Train RMSE: 0.4944, Val Loss: 0.0512, Val MAE: 0.1739, Val RMSE: 0.2210\n",
      "No improvement, patience left: 4\n",
      "Epoch 20/100, Train Loss: 0.3346, Train MAE: 0.4262, Train RMSE: 0.4932, Val Loss: 0.0502, Val MAE: 0.1702, Val RMSE: 0.2185\n",
      "No improvement, patience left: 3\n",
      "Epoch 21/100, Train Loss: 0.3367, Train MAE: 0.4280, Train RMSE: 0.4951, Val Loss: 0.0525, Val MAE: 0.1792, Val RMSE: 0.2240\n",
      "No improvement, patience left: 2\n",
      "Epoch 22/100, Train Loss: 0.3363, Train MAE: 0.4263, Train RMSE: 0.4930, Val Loss: 0.0517, Val MAE: 0.1752, Val RMSE: 0.2221\n",
      "No improvement, patience left: 1\n",
      "Epoch 23/100, Train Loss: 0.3364, Train MAE: 0.4275, Train RMSE: 0.4942, Val Loss: 0.0518, Val MAE: 0.1758, Val RMSE: 0.2222\n",
      "No improvement, patience left: 0\n",
      "Early Stopping Triggered after epoch 23\n",
      "Training Completed in 81.97 seconds\n",
      "Loaded checkpoint from models/best_cpg_model_advanced_packpad.pth, resuming from epoch 13 with best validation loss: 0.0269\n"
     ]
    }
   ],
   "source": [
    "# Normal model training\n",
    "trained_model = train_model_pack_padded(\n",
    "    new_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    epochs=num_epochs,\n",
    "    patience=stop_patience,\n",
    "    save_path=\"best_cpg_model_advanced_packpad.pth\",\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACNTCANCNCNTNGCGANTNGGGNATGGGTTANNTNNGGGNCTTCGCTANTCATTTAANCTGCGATTGGNGNTGCCNTATTTNCGACAANCTGTGCACGCCTNCGNCTNA\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# create a test dataset\n",
    "test_dna, count_cpg = create_test_data()\n",
    "print(test_dna)\n",
    "print(count_cpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voila! The model is working perfectly\n",
      "DNA: ACNTCANCNCNTNGCGANTNGGGNATGGGTTANNTNNGGGNCTTCGCTANTCATTTAANCTGCGATTGGNGNTGCCNTATTTNCGACAANCTGTGCACGCCTNCGNCTNA \n",
      "🔹 Predicted CpG Count: 5.85\n"
     ]
    }
   ],
   "source": [
    "# test prediction from the advanced model\n",
    "predicted_cpgs = predict_cpgs_from_dna_pack_padded(\n",
    "    \"best_cpg_model_advanced_packpad.pth\",\n",
    "    test_dna,\n",
    "    dna2int,\n",
    "    embedding_dim,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    device,\n",
    "    model_class=CpGCounterAdvancedPackPadding,\n",
    ")\n",
    "print(\"Voila! The model is working perfectly\")\n",
    "print(f\"DNA: {test_dna} \\n🔹 Predicted CpG Count: {predicted_cpgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -------------- Hyperparameter Tuning ------------------ #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-02-03 17:01:49,665]\u001b[0m A new study created in memory with name: cpg_optuna\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-02-03 17:02:37,317]\u001b[0m Trial 0 finished with value: 0.28130437107756734 and parameters: {'embedding_dim': 128, 'hidden_size': 512, 'num_layers': 1, 'learning_rate': 0.00010051188891410282, 'weight_decay': 0.0011020350108512757, 'dropout': 0.18977875104643538}. Best is trial 0 with value: 0.28130437107756734.\u001b[0m\n",
      "\u001b[32m[I 2025-02-03 17:03:20,907]\u001b[0m Trial 1 finished with value: 0.03693105326965451 and parameters: {'embedding_dim': 256, 'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.00038663647849379747, 'weight_decay': 0.0003332445708937284, 'dropout': 0.15203537828834462}. Best is trial 1 with value: 0.03693105326965451.\u001b[0m\n",
      "\u001b[32m[I 2025-02-03 17:05:18,349]\u001b[0m Trial 2 finished with value: 0.029716629622271284 and parameters: {'embedding_dim': 64, 'hidden_size': 512, 'num_layers': 3, 'learning_rate': 0.0004204614036578065, 'weight_decay': 0.0002772933970987665, 'dropout': 0.2335641535286226}. Best is trial 2 with value: 0.029716629622271284.\u001b[0m\n",
      "\u001b[32m[I 2025-02-03 17:06:02,002]\u001b[0m Trial 3 finished with value: 0.041312347224447876 and parameters: {'embedding_dim': 64, 'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001220668517247448, 'weight_decay': 0.00127019046243005, 'dropout': 0.24457204356078371}. Best is trial 2 with value: 0.029716629622271284.\u001b[0m\n",
      "\u001b[32m[I 2025-02-03 17:07:10,751]\u001b[0m Trial 4 finished with value: 0.18019674869719893 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0002420566821253116, 'weight_decay': 8.254241731514292e-05, 'dropout': 0.20898499930219167}. Best is trial 2 with value: 0.029716629622271284.\u001b[0m\n",
      "\u001b[32m[I 2025-02-03 17:08:21,506]\u001b[0m Trial 5 finished with value: 0.033480492420494556 and parameters: {'embedding_dim': 128, 'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.004615801615121536, 'weight_decay': 0.0002515453920791827, 'dropout': 0.11959536995543898}. Best is trial 2 with value: 0.029716629622271284.\u001b[0m\n",
      "\u001b[32m[I 2025-02-03 17:09:32,473]\u001b[0m Trial 6 finished with value: 0.007654339591681492 and parameters: {'embedding_dim': 256, 'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0012714184201257357, 'weight_decay': 0.0014472616603016003, 'dropout': 0.20122992106623538}. Best is trial 6 with value: 0.007654339591681492.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'embedding_dim': 256, 'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0012714184201257357, 'weight_decay': 0.0014472616603016003, 'dropout': 0.20122992106623538}\n",
      "Starting training from scratch.\n",
      "Epoch 1/100, Train Loss: 7.3238, Train MAE: 2.1066, Train RMSE: 2.6331, Val Loss: 5.0425, Val MAE: 1.8097, Val RMSE: 2.2140\n",
      "New best validation loss: 5.0425 (previous best: inf)\n",
      "Model checkpoint saved at models/best_cpg_model_optuna.pth\n",
      "Epoch 2/100, Train Loss: 5.8061, Train MAE: 1.8643, Train RMSE: 2.3506, Val Loss: 5.1400, Val MAE: 1.8439, Val RMSE: 2.2400\n",
      "No improvement, patience left: 9\n",
      "Epoch 3/100, Train Loss: 5.4962, Train MAE: 1.8032, Train RMSE: 2.2798, Val Loss: 5.6980, Val MAE: 2.0648, Val RMSE: 2.3682\n",
      "No improvement, patience left: 8\n",
      "Epoch 4/100, Train Loss: 3.3540, Train MAE: 1.2412, Train RMSE: 1.7313, Val Loss: 7.0657, Val MAE: 2.2085, Val RMSE: 2.6395\n",
      "No improvement, patience left: 7\n",
      "Epoch 5/100, Train Loss: 1.0630, Train MAE: 0.7489, Train RMSE: 0.9544, Val Loss: 0.5767, Val MAE: 0.6431, Val RMSE: 0.7530\n",
      "New best validation loss: 0.5767 (previous best: 5.0425)\n",
      "Model checkpoint saved at models/best_cpg_model_optuna.pth\n",
      "Epoch 6/100, Train Loss: 0.5210, Train MAE: 0.5365, Train RMSE: 0.6608, Val Loss: 1.3387, Val MAE: 0.9564, Val RMSE: 1.1457\n",
      "No improvement, patience left: 9\n",
      "Epoch 7/100, Train Loss: 0.4698, Train MAE: 0.5121, Train RMSE: 0.6239, Val Loss: 0.2170, Val MAE: 0.3657, Val RMSE: 0.4564\n",
      "New best validation loss: 0.2170 (previous best: 0.5767)\n",
      "Model checkpoint saved at models/best_cpg_model_optuna.pth\n",
      "Epoch 8/100, Train Loss: 0.4456, Train MAE: 0.5000, Train RMSE: 0.6017, Val Loss: 0.3428, Val MAE: 0.5179, Val RMSE: 0.5823\n",
      "No improvement, patience left: 9\n",
      "Epoch 9/100, Train Loss: 0.4512, Train MAE: 0.4949, Train RMSE: 0.6015, Val Loss: 0.1061, Val MAE: 0.2772, Val RMSE: 0.3217\n",
      "New best validation loss: 0.1061 (previous best: 0.2170)\n",
      "Model checkpoint saved at models/best_cpg_model_optuna.pth\n",
      "Epoch 10/100, Train Loss: 0.4350, Train MAE: 0.4917, Train RMSE: 0.5869, Val Loss: 0.2125, Val MAE: 0.4107, Val RMSE: 0.4587\n",
      "No improvement, patience left: 9\n",
      "Epoch 11/100, Train Loss: 0.4213, Train MAE: 0.4826, Train RMSE: 0.5733, Val Loss: 0.1716, Val MAE: 0.3836, Val RMSE: 0.4125\n",
      "No improvement, patience left: 8\n",
      "Epoch 12/100, Train Loss: 0.4026, Train MAE: 0.4752, Train RMSE: 0.5586, Val Loss: 0.1878, Val MAE: 0.3997, Val RMSE: 0.4318\n",
      "No improvement, patience left: 7\n",
      "Epoch 13/100, Train Loss: 0.3424, Train MAE: 0.4444, Train RMSE: 0.5095, Val Loss: 0.0495, Val MAE: 0.1514, Val RMSE: 0.2170\n",
      "New best validation loss: 0.0495 (previous best: 0.1061)\n",
      "Model checkpoint saved at models/best_cpg_model_optuna.pth\n",
      "Epoch 14/100, Train Loss: 0.3307, Train MAE: 0.4341, Train RMSE: 0.4922, Val Loss: 0.0421, Val MAE: 0.1421, Val RMSE: 0.2006\n",
      "New best validation loss: 0.0421 (previous best: 0.0495)\n",
      "Model checkpoint saved at models/best_cpg_model_optuna.pth\n",
      "Epoch 15/100, Train Loss: 0.3271, Train MAE: 0.4331, Train RMSE: 0.4893, Val Loss: 0.0500, Val MAE: 0.1508, Val RMSE: 0.2181\n",
      "No improvement, patience left: 9\n",
      "Epoch 16/100, Train Loss: 0.3240, Train MAE: 0.4318, Train RMSE: 0.4857, Val Loss: 0.0416, Val MAE: 0.1534, Val RMSE: 0.2006\n",
      "New best validation loss: 0.0416 (previous best: 0.0421)\n",
      "Model checkpoint saved at models/best_cpg_model_optuna.pth\n",
      "Epoch 17/100, Train Loss: 0.3205, Train MAE: 0.4278, Train RMSE: 0.4812, Val Loss: 0.0293, Val MAE: 0.1261, Val RMSE: 0.1678\n",
      "New best validation loss: 0.0293 (previous best: 0.0416)\n",
      "Model checkpoint saved at models/best_cpg_model_optuna.pth\n",
      "Epoch 18/100, Train Loss: 0.3167, Train MAE: 0.4271, Train RMSE: 0.4785, Val Loss: 0.0342, Val MAE: 0.1311, Val RMSE: 0.1807\n",
      "No improvement, patience left: 9\n",
      "Epoch 19/100, Train Loss: 0.3135, Train MAE: 0.4259, Train RMSE: 0.4763, Val Loss: 0.0369, Val MAE: 0.1416, Val RMSE: 0.1880\n",
      "No improvement, patience left: 8\n",
      "Epoch 20/100, Train Loss: 0.3143, Train MAE: 0.4245, Train RMSE: 0.4762, Val Loss: 0.0447, Val MAE: 0.1560, Val RMSE: 0.2071\n",
      "No improvement, patience left: 7\n",
      "Epoch 21/100, Train Loss: 0.3104, Train MAE: 0.4216, Train RMSE: 0.4716, Val Loss: 0.0420, Val MAE: 0.1661, Val RMSE: 0.2013\n",
      "No improvement, patience left: 6\n",
      "Epoch 22/100, Train Loss: 0.3080, Train MAE: 0.4210, Train RMSE: 0.4700, Val Loss: 0.0426, Val MAE: 0.1678, Val RMSE: 0.2028\n",
      "No improvement, patience left: 5\n",
      "Epoch 23/100, Train Loss: 0.3071, Train MAE: 0.4196, Train RMSE: 0.4676, Val Loss: 0.0421, Val MAE: 0.1647, Val RMSE: 0.2012\n",
      "No improvement, patience left: 4\n",
      "Epoch 24/100, Train Loss: 0.3067, Train MAE: 0.4197, Train RMSE: 0.4675, Val Loss: 0.0426, Val MAE: 0.1677, Val RMSE: 0.2028\n",
      "No improvement, patience left: 3\n",
      "Epoch 25/100, Train Loss: 0.3077, Train MAE: 0.4194, Train RMSE: 0.4675, Val Loss: 0.0400, Val MAE: 0.1604, Val RMSE: 0.1963\n",
      "No improvement, patience left: 2\n",
      "Epoch 26/100, Train Loss: 0.3070, Train MAE: 0.4199, Train RMSE: 0.4675, Val Loss: 0.0410, Val MAE: 0.1625, Val RMSE: 0.1985\n",
      "No improvement, patience left: 1\n",
      "Epoch 27/100, Train Loss: 0.3067, Train MAE: 0.4199, Train RMSE: 0.4668, Val Loss: 0.0424, Val MAE: 0.1658, Val RMSE: 0.2021\n",
      "No improvement, patience left: 0\n",
      "Early Stopping Triggered after epoch 27\n",
      "Training Completed in 99.71 seconds\n",
      "Loaded checkpoint from models/best_cpg_model_optuna.pth, resuming from epoch 17 with best validation loss: 0.0293\n"
     ]
    }
   ],
   "source": [
    "# Run Hyperparameter Tuning\n",
    "best_hyperparams, trained_model = tune_hyperparameters(\n",
    "    vocab_size,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    stop_patience,\n",
    "    n_trials=7,\n",
    "    save_best_model_path=\"best_cpg_model_optuna.pth\",\n",
    "    study_name=\"cpg_optuna\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voila! The hyper tuned model is working perfectly\n",
      "DNA: ACNTCANCNCNTNGCGANTNGGGNATGGGTTANNTNNGGGNCTTCGCTANTCATTTAANCTGCGATTGGNGNTGCCNTATTTNCGACAANCTGTGCACGCCTNCGNCTNA \n",
      "🔹 Predicted CpG Count: 5.67\n"
     ]
    }
   ],
   "source": [
    "# test prediction from the advanced model\n",
    "predicted_cpgs = predict_cpgs_from_dna_pack_padded(\n",
    "    model_path=\"best_cpg_model_optuna.pth\",\n",
    "    dna_sequence=test_dna,\n",
    "    dna2int=dna2int,\n",
    "    embedding_dim=best_hyperparams[\"embedding_dim\"],\n",
    "    hidden_size=best_hyperparams[\"hidden_size\"],\n",
    "    num_layers=best_hyperparams[\"num_layers\"],\n",
    "    dropout=best_hyperparams[\"dropout\"],\n",
    "    device=device,\n",
    "    model_class=CpGCounterAdvancedPackPadding,\n",
    ")\n",
    "print(\"Voila! The hyper tuned model is working perfectly\")\n",
    "print(f\"DNA: {test_dna} \\n🔹 Predicted CpG Count: {predicted_cpgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################## END OF SETUP ################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emlo_env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
