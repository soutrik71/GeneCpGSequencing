{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/soutrik-gpuvm/code/Users/Soutrik.Chowdhury/GeneCpGSequencing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../.\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we will experiment with rnn padding sequences concepts like padding, packing and unpacking sequences\n",
    "- Sequence Padding and Packing for RNNs\n",
    "- Implementation of Sequence Padding and Sequence Packing\n",
    "- Handling Sequence Padding and Packing in PyTorch for RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where padding comes and pad all sequences to the maximum length (8 in this case) with meaningless values. This creates an 8x8 matrix for computations, even though some sequences are shorter. This wastes processing power because we perform unnecessary calculations (64 computations instead of the actual 45 needed).\n",
    "\n",
    "For this , packing plays an important role as It packs the sequences into a data structure that preserves their original lengths before padding. By doing so, the RNN model can process only the non-padded portions of each sequence, effectively reducing the computational overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "# Define sequences\n",
    "sequences = [[1, 2, 3, 7], [4, 5], [6, 7, 8, 9, 11], [10]]\n",
    "sequences_tensor = [\n",
    "    torch.tensor(seq) for seq in sequences\n",
    "]  # Convert sequences to PyTorch tensors\n",
    "sequences_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences: \n",
      " tensor([[ 1,  2,  3,  7,  0],\n",
      "        [ 4,  5,  0,  0,  0],\n",
      "        [ 6,  7,  8,  9, 11],\n",
      "        [10,  0,  0,  0,  0]])\n",
      "Padded sequences shape: \n",
      " torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# Padding : pad_sequence function from torch.nn.utils.rnn module to pad the sequences to the same length\n",
    "padded_sequences = rnn_utils.pad_sequence(sequences_tensor, batch_first=True)\n",
    "print(\"Padded sequences:\", \"\\n\", padded_sequences)\n",
    "print(\n",
    "    \"Padded sequences shape:\", \"\\n\", padded_sequences.shape\n",
    ")  # (4, 5) : 4 sequences, each of length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Packed sequences: PackedSequence(data=tensor([ 6,  1,  4, 10,  7,  2,  5,  8,  3,  9,  7, 11]), batch_sizes=tensor([4, 3, 2, 2, 1]), sorted_indices=tensor([2, 0, 1, 3]), unsorted_indices=tensor([1, 2, 0, 3]))\n"
     ]
    }
   ],
   "source": [
    "# Packing : pack_padded_sequence function from torch.nn.utils.rnn module to pack the padded sequences into a packed sequence object\n",
    "sequence_lengths = torch.tensor(\n",
    "    [len(seq) for seq in sequences]\n",
    ")  # Calculating actual lengths of sequences before padding\n",
    "# Pack padded sequences into a packed sequence object\n",
    "packed_sequences = rnn_utils.pack_padded_sequence(\n",
    "    padded_sequences, sequence_lengths, batch_first=True, enforce_sorted=False\n",
    ")\n",
    "print(\"\\nPacked sequences:\", packed_sequences)\n",
    "# Packed sequences object contains data, batch_sizes, sorted_indices, and unsorted_indices\n",
    "# row wise data is first sorted by length and then packed into a single tensor by column wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How pad sequence works is that it takes a list of sequences, and pads them to the same length. The sequences are expected to be in the form of PyTorch tensors. The pad_sequence function returns a tensor with the padded sequences. The sequences are padded with zeros by default. The padding is done at the beginning of the sequences. The pad_sequence function also takes an optional batch_first argument, which specifies whether the batch dimension should be the first dimension of the output tensor. If batch_first is set to True, the output tensor will have the batch dimension as the first dimension. If batch_first is set to False, the output tensor will have the batch dimension as the second dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the packed sequence:\n",
    "- data: contains the flattened non-padded elements from the padded sequences.\n",
    "- batch_sizes: indicates how many elements are present at each time step, reflecting the varying sequence lengths within the batch.\n",
    "- This packed sequence is feed into your recurrent neural network (RNN) model during training, allowing it to efficiently process variable-length sequences.\n",
    "- How it works is that the RNN model processes the packed sequence element by element, using the batch_sizes tensor to keep track of the sequence lengths. The RNN model skips the padding elements and only processes the actual sequence elements, reducing the computational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will show with real world example of using pack_padded_sequence and pad_packed_sequence functions in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implementing Padding and Packing in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1, 2]), tensor([3]), tensor([4, 5, 6, 7])]\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Example sequences (already encoded as numbers)\n",
    "sequences = [\n",
    "    torch.tensor([1, 2]),  # \"Hello\"\n",
    "    torch.tensor([3]),  # \"Hi\"\n",
    "    torch.tensor([4, 5, 6, 7]),  # \"Goodbye\"\n",
    "]\n",
    "print(sequences)\n",
    "print(sequences[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([4, 5, 6, 7]), tensor([1, 2]), tensor([3])]\n",
      "Padded Sequences:\n",
      " tensor([[4, 5, 6, 7],\n",
      "        [1, 2, 0, 0],\n",
      "        [3, 0, 0, 0]])\n",
      "Sequence Lengths:\n",
      " tensor([4, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# Sort sequences by length (descending order required for packing)\n",
    "sequences.sort(key=len, reverse=True)\n",
    "print(sequences)\n",
    "\n",
    "# Extract lengths before padding (important for packing)\n",
    "sequence_lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "\n",
    "# Pad sequences to match the longest one\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "\n",
    "print(\"Padded Sequences:\\n\", padded_sequences)\n",
    "print(\"Sequence Lengths:\\n\", sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed Sequences:\n",
      " PackedSequence(data=tensor([4, 1, 3, 5, 2, 6, 7]), batch_sizes=tensor([3, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "# Pack the padded sequence\n",
    "packed_sequences = pack_padded_sequence(\n",
    "    padded_sequences, sequence_lengths, batch_first=True, enforce_sorted=True\n",
    ")\n",
    "\n",
    "print(\"Packed Sequences:\\n\", packed_sequences)\n",
    "# Packed sequences object contains data, batch_sizes, sorted_indices, and unsorted_indices\n",
    "# row wise data is first sorted by length and then packed into a single tensor by column\n",
    "# batch_sizes contains the number of elements in each batch at each time step which gives the actual length of each sequence without padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4, 5, 6, 7],\n",
       "         [1, 2, 0, 0],\n",
       "         [3, 0, 0, 0]]),\n",
       " tensor([4, 2, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_packed_sequence(packed_sequences, batch_first=True)\n",
    "# pad_packed_sequence function is used to unpack the packed sequence object back into a padded sequence\n",
    "# this will be useful when we want to use the output of an RNN layer in a subsequent layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Packing Helps?\n",
    "* It ignores padded values during computation.\n",
    "* The RNN only processes valid time steps, improving efficiency and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model that takes a sequence of embeddings and returns the output of the LSTM layer\n",
    "    and the final hidden state of the LSTM layer (h_n) for each sequence in the batch.\n",
    "    Uses pack_padded_sequence and pad_packed_sequence to handle variable-length sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embed token IDs\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, batch_first=True\n",
    "        )  # Fix input size\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(\n",
    "            x\n",
    "        )  # Output shape: (batch_size, seq_len, embedding_dim)\n",
    "        print(\"Embedded Shape:\", embedded.shape)\n",
    "\n",
    "        # Pack the sequence\n",
    "        packed_x = pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "        print(\"Packed X Shape:\", packed_x.data.shape)\n",
    "\n",
    "        # Process with LSTM\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_x)\n",
    "        print(\"Packed Output Shape:\", packed_output.data.shape)\n",
    "\n",
    "        # Unpack the output\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        print(\"Unpacked Output Shape:\", output.shape)\n",
    "\n",
    "        return output, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Sequences:\n",
      " tensor([[ 6,  7,  8,  9],\n",
      "        [ 1,  2,  3,  0],\n",
      "        [ 4,  5,  0,  0],\n",
      "        [10,  0,  0,  0]])\n",
      "Sequence Lengths:\n",
      " tensor([4, 3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Data Preparation ------------------------\n",
    "\n",
    "# Example tokenized sequences (variable-length)\n",
    "sequences = [\n",
    "    torch.tensor([1, 2, 3]),\n",
    "    torch.tensor([4, 5]),\n",
    "    torch.tensor([6, 7, 8, 9]),\n",
    "    torch.tensor([10]),\n",
    "]\n",
    "\n",
    "# Sort sequences by length in descending order (required for packing)\n",
    "sequences.sort(key=len, reverse=True)\n",
    "\n",
    "# Pad sequences to match the longest one\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "sequence_lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "\n",
    "print(\"Padded Sequences:\\n\", padded_sequences)\n",
    "print(\"Sequence Lengths:\\n\", sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Shape: torch.Size([4, 4, 100])\n",
      "Packed X Shape: torch.Size([10, 100])\n",
      "Packed Output Shape: torch.Size([10, 256])\n",
      "Unpacked Output Shape: torch.Size([4, 4, 256])\n",
      "Final Output Shape: torch.Size([4, 4, 256])\n",
      "Final Hidden State Shape: torch.Size([1, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Model Initialization ------------------------\n",
    "\n",
    "VOCAB_SIZE = 11  # Number of unique tokens (including padding)\n",
    "EMBEDDING_DIM = 100  # Embedding vector size\n",
    "HIDDEN_DIM = 256  # LSTM hidden state size\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "\n",
    "# ------------------------ Forward Pass ------------------------\n",
    "\n",
    "# Pass the padded sequences through the model\n",
    "outputs, hidden_state = model(padded_sequences, sequence_lengths)\n",
    "\n",
    "print(\"Final Output Shape:\", outputs.shape)\n",
    "print(\"Final Hidden State Shape:\", hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we will deep dive into pack_padded_sequence and pad_packed_sequence functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPG Counting using LSTM\n",
    "This project involves building a neural network to count the number of CpGs (consecutive CGs) in DNA sequences. Here's a structured solution for this problem, from problem understanding to advanced implementations using LSTMs, PyTorch optimizations, and hyperparameter tuning with Optuna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed=13):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA\")\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "# DNA Alphabet Encoding\n",
    "alphabet = \"NACGT\"\n",
    "dna2int = {a: i for a, i in zip(alphabet, range(1, 6))}  # Map A, C, G, T, N → [1,5]\n",
    "int2dna = {i: a for a, i in zip(alphabet, range(1, 6))}  # Reverse mapping\n",
    "dna2int.update({\"pad\": 0})  # Add padding token\n",
    "int2dna.update({0: \"<pad>\"})  # Padding representation\n",
    "\n",
    "# Mapping Functions\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)\n",
    "list(intseq_to_dnaseq([1]))\n",
    "print(list(dnaseq_to_intseq([\"C\"])))\n",
    "print(list(dnaseq_to_intseq([\"G\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 1, 'A': 2, 'C': 3, 'G': 4, 'T': 5, 'pad': 0}\n",
      "{1: 'N', 2: 'A', 3: 'C', 4: 'G', 5: 'T', 0: '<pad>'}\n"
     ]
    }
   ],
   "source": [
    "print(dna2int)\n",
    "print(int2dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count CpGs in a DNA sequence\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of CpG dinucleotides in a DNA sequence.\n",
    "    \"\"\"\n",
    "    return sum(1 for i in range(len(seq) - 1) if seq[i : i + 2] == \"CG\")\n",
    "\n",
    "\n",
    "# Unified function to generate data\n",
    "def generate_dna_data(\n",
    "    n_seqs: int, fixed_len: int = None, lb: int = 16, ub: int = 128\n",
    ") -> Tuple[List[List[int]], List[int]]:\n",
    "    \"\"\"\n",
    "    Generate DNA sequences and their CpG counts.\n",
    "\n",
    "    Parameters:\n",
    "    - n_seqs (int): Number of sequences to generate.\n",
    "    - fixed_len (int, optional): If given, generates fixed-length sequences.\n",
    "    - lb (int): Lower bound for sequence length (used if fixed_len is None).\n",
    "    - ub (int): Upper bound for sequence length (used if fixed_len is None).\n",
    "\n",
    "    Returns:\n",
    "    - X (List[List[int]]): List of integer-encoded DNA sequences.\n",
    "    - y (List[int]): List of CpG counts corresponding to each sequence.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    for _ in range(n_seqs):\n",
    "        seq_len = (\n",
    "            fixed_len if fixed_len else random.randint(lb, ub)\n",
    "        )  # Choose sequence length\n",
    "        int_seq = [\n",
    "            random.randint(1, 5) for _ in range(seq_len)\n",
    "        ]  # Generate integer-encoded DNA sequence\n",
    "        dna_seq = \"\".join(intseq_to_dnaseq(int_seq))  # Convert to DNA sequence\n",
    "\n",
    "        X.append(int_seq)  # Store integer-encoded sequence\n",
    "        y.append(count_cpgs(dna_seq))  # Store CpG count\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fixed-length dataset (128 nucleotides per sequence)\n",
    "X_fixed, y_fixed = generate_dna_data(n_seqs=1000, fixed_len=32)\n",
    "\n",
    "# Generate variable-length dataset (random lengths between 16 and 128)\n",
    "X_variable, y_variable = generate_dna_data(n_seqs=1000, lb=16, ub=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_fixed), len(X_variable)\n",
    "len(y_fixed), len(y_variable)\n",
    "\n",
    "min(map(len, X_fixed)), max(map(len, X_fixed))\n",
    "min(map(len, X_variable)), max(map(len, X_variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_fixed[0],y_fixed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------------------------ Hyperparameters ------------------------\n",
    "batch_size = 16\n",
    "embedding_dim = 32\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------ Pytorch Dataset ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from typing import Tuple, List\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will test with same length sequences and then with variable length sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 100\n",
      "900 100\n"
     ]
    }
   ],
   "source": [
    "# Split variable-length dataset (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_fixed, y_fixed, test_size=0.1, random_state=13\n",
    ")\n",
    "print(len(X_train), len(X_val))\n",
    "print(len(y_train), len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPGDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with sequences and labels (CpG counts) as input arguments and store them as attributes.\n",
    "\n",
    "        Parameters:\n",
    "        - sequences (List[List[int]]): List of integer-encoded DNA sequences.\n",
    "        - labels (List[int]): List of CpG counts corresponding to each sequence.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        self.sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a sequence and its label by index.\n",
    "        \"\"\"\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to pad sequences dynamically and return a batch of sequences and labels.\n",
    "        \"\"\"\n",
    "        sequences, labels = zip(*batch)\n",
    "        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "        return padded_sequences, torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets and dataloaders for fixed and variable-length sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch standard dataset\n",
    "train_dataset = CPGDataset(X_train, y_train)\n",
    "val_dataset = CPGDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# each iteration of the dataset will return a list of sequences and a labels\n",
    "x, y = next(iter(train_dataset))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=CPGDataset.collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=CPGDataset.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32]) torch.Size([16])\n",
      "torch.Size([16, 32]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# each iteration of the dataloader will return a batch of sequences and labels\n",
    "for x_batch, y_batch in train_dataloader:\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break\n",
    "\n",
    "for x_batch, y_batch in val_dataloader:\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Batch Shape: torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "# Sample batch\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(\"Padded Batch Shape:\", sample_batch[0].shape)  # (batch_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ------------------------ Model Initialization ------------------------\n",
    "The model consists of:\n",
    "\n",
    "- Embedding Layer – Converts integer-encoded DNA sequences into dense vector representations.\n",
    "- LSTM Layer – Captures sequential patterns in DNA.\n",
    "- Fully Connected Layer – Outputs the CpG count as a single continuous value.\n",
    "- ReLU Activation – Ensures non-negative predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Summary of Shape Transformations**\n",
    "\n",
    "| Step | Layer                          | Input Shape                     | Output Shape                    | Explanation |\n",
    "|------|--------------------------------|---------------------------------|---------------------------------|-------------|\n",
    "| 1️⃣  | **Raw Input**                  | `(batch_size, seq_len)`         | `(4, 10)`                        | Raw integer-encoded DNA sequences, where `batch_size = 4` (number of sequences in a batch) and `seq_len = 10` (length of each sequence). |\n",
    "| 2️⃣  | **Embedding**                  | `(batch_size, seq_len)`         | `(4, 10, 64)`                     | Converts integer tokens into dense vectors of size `embedding_dim = 64`. This means each nucleotide is mapped to a 64-dimensional vector. |\n",
    "| 3️⃣  | **LSTM**                        | `(batch_size, seq_len, embedding_dim)` | `(4, 10, 128)`          | The LSTM processes the sequence and outputs a hidden state of size `hidden_size = 128` for each time step in the sequence. |\n",
    "| 4️⃣  | **Selecting Last Hidden State** | `(batch_size, seq_len, hidden_size)` | `(4, 128)`             | Extracts the last time step's hidden state (`lstm_output[:, -1, :]`), which summarizes the sequence information. |\n",
    "| 5️⃣  | **Fully Connected Layer**       | `(batch_size, hidden_size)`     | `(4, 1)`                         | Linear transformation reduces `hidden_size = 128` to a single output value per sequence (predicting the CpG count). |\n",
    "| 6️⃣  | **ReLU Activation**             | `(batch_size, 1)`               | `(4, 1)`                         | Ensures non-negative predictions for CpG site counts. |\n",
    "\n",
    "\n",
    "### **Hyperparameters Used in the Model**\n",
    "- **`batch_size = 4`** → Number of sequences in a batch.\n",
    "- **`seq_len = 10`** → Length of each DNA sequence.\n",
    "- **`vocab_size = 11`** → Total number of unique nucleotides (A, C, G, T) including padding.\n",
    "- **`embedding_dim = 64`** → Size of the dense representation of each nucleotide.\n",
    "- **`hidden_size = 128`** → Number of hidden units in the LSTM layer.\n",
    "- **`num_layers = 2`** → Number of stacked LSTM layers.\n",
    "- **`dropout = 0.3`** → Dropout rate for regularization in LSTM.\n",
    "\n",
    "This breakdown ensures **clear understanding of shape transformations** throughout the model. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CpGCounter(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(CpGCounter, self).__init__()\n",
    "        # Embedding layer to convert integer-encoded sequences to embeddings of fixed size (embedding_dim)\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=0\n",
    "        )  # input: (batch_size, seq_len), output: (batch_size, seq_len, embedding_dim)\n",
    "        # LSTM layer to process the embeddings and capture sequential information (hidden_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout\n",
    "        )  # input: (batch_size, seq_len, embedding_dim), output: (batch_size, seq_len, hidden_size)\n",
    "        # Fully connected layer to predict the number of CpG sites\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_size, 1\n",
    "        )  # input: (batch_size, seq_len, hidden_size), output: (batch_size, seq_len, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed the input sequences\n",
    "        embedded = self.embedding(x)\n",
    "        # Process the embeddings with an LSTM layer\n",
    "        lstm_output, _ = self.lstm(embedded)\n",
    "        # we only take the last hidden state\n",
    "        lstm_output = lstm_output[\n",
    "            :, -1, :\n",
    "        ]  # shape is (batch_size, hidden_size) and we only take the last hidden state of the sequence\n",
    "        # Predict the number of CpG sites\n",
    "        output = self.fc(lstm_output)\n",
    "        return self.relu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# check cuda availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(alphabet) + 1  # Add 1 for padding token\n",
    "print(vocab_size)\n",
    "# print(len(int2dna))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only quick testing of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize model\n",
    "# model = CpGCounter(\n",
    "#     vocab_size=vocab_size,\n",
    "#     embedding_dim=embedding_dim,\n",
    "#     hidden_size=hidden_size,\n",
    "#     num_layers=num_layers,\n",
    "#     dropout=dropout,\n",
    "# ).to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define loss function and optimizer and scheduler\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer, mode=\"min\", factor=0.1, patience=5, verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(\n",
    "#     train_dataloader\n",
    "# )  # 50 sets of batch and each batch has 16 sequences with each sequence having 32 nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0.0\n",
    "#     all_preds, all_labels = [], []  # Store predictions and labels\n",
    "#     model.train()\n",
    "\n",
    "#     for inputs, labels in train_dataloader:\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs).squeeze()  # Remove extra dimension\n",
    "\n",
    "#         loss = criterion(outputs, labels)  # Compute loss\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(\n",
    "#             model.parameters(), max_norm=1.0\n",
    "#         )  # Gradient Clipping\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Store predictions and labels for metrics\n",
    "#         all_preds.extend(outputs.detach().cpu().numpy())\n",
    "#         all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "#     # Scheduler Step\n",
    "#     scheduler.step(total_loss)\n",
    "\n",
    "#     # Compute Regression Metrics (if applicable)\n",
    "#     mae = mean_absolute_error(all_labels, all_preds)\n",
    "#     mse = mean_squared_error(all_labels, all_preds)\n",
    "#     rmse = mse**0.5  # Root Mean Squared Error\n",
    "\n",
    "#     print(\n",
    "#         f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will improve the training loop by adding \n",
    "- validation and early stopping to prevent overfitting \n",
    "- saving the best model\n",
    "- more functionalities like logging, plotting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, dataloader, device, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Runs one training epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): PyTorch model to train.\n",
    "    - dataloader (DataLoader): Training DataLoader.\n",
    "    - device (torch.device): CPU or GPU.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "\n",
    "    Returns:\n",
    "    - avg_loss (float): Average loss over dataset.\n",
    "    - avg_mae (float): Average MAE over dataset.\n",
    "    - avg_rmse (float): Average RMSE over dataset.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mae, total_rmse = 0.0, 0.0, 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = (\n",
    "            inputs.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute batch-wise metrics\n",
    "        batch_mae = mean_absolute_error(\n",
    "            labels.cpu().numpy(), outputs.detach().cpu().numpy()\n",
    "        )\n",
    "        batch_rmse = (\n",
    "            mean_squared_error(labels.cpu().numpy(), outputs.detach().cpu().numpy())\n",
    "            ** 0.5\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mae += batch_mae\n",
    "        total_rmse += batch_rmse\n",
    "\n",
    "    return total_loss / num_batches, total_mae / num_batches, total_rmse / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, dataloader, device, criterion):\n",
    "    \"\"\"\n",
    "    Runs one validation epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): PyTorch model to evaluate.\n",
    "    - dataloader (DataLoader): Validation DataLoader.\n",
    "    - device (torch.device): CPU or GPU.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "\n",
    "    Returns:\n",
    "    - avg_loss (float): Average loss over dataset.\n",
    "    - avg_mae (float): Average MAE over dataset.\n",
    "    - avg_rmse (float): Average RMSE over dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_mae, total_rmse = 0.0, 0.0, 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = (\n",
    "                inputs.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            outputs = model(inputs).squeeze()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute batch-wise metrics\n",
    "            batch_mae = mean_absolute_error(labels.cpu().numpy(), outputs.cpu().numpy())\n",
    "            batch_rmse = (\n",
    "                mean_squared_error(labels.cpu().numpy(), outputs.cpu().numpy()) ** 0.5\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae += batch_mae\n",
    "            total_rmse += batch_rmse\n",
    "\n",
    "    return total_loss / num_batches, total_mae / num_batches, total_rmse / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    epoch, model, optimizer, scheduler, best_val_loss, save_path=\"best_cpg_model.pth\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves the model, optimizer, and scheduler state for training resumption.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch + 1,  # Save next epoch to resume correctly\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model checkpoint saved at {save_path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    model, optimizer, scheduler, device, save_path=\"best_cpg_model.pth\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a saved checkpoint to resume training.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(save_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "\n",
    "    print(\n",
    "        f\"Loaded checkpoint from {save_path}, resuming from epoch {checkpoint['epoch']} with best validation loss: {best_val_loss:.4f}\"\n",
    "    )\n",
    "    return model, optimizer, scheduler, checkpoint[\"epoch\"], best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    epochs=25,\n",
    "    patience=5,\n",
    "    save_path=\"best_cpg_model.pth\",\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    resume=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains an LSTM model with validation and early stopping. Supports resuming training.\n",
    "\n",
    "    Parameters:\n",
    "    - model: LSTM model.\n",
    "    - train_loader: Training DataLoader.\n",
    "    - val_loader: Validation DataLoader.\n",
    "    - device: CPU or GPU.\n",
    "    - epochs: Max training epochs.\n",
    "    - patience: Early stopping patience.\n",
    "    - save_path: Path to save the best model.\n",
    "    - lr: Initial learning rate.\n",
    "    - weight_decay: L2 regularization weight.\n",
    "    - resume: Whether to resume training from the last checkpoint.\n",
    "\n",
    "    Returns:\n",
    "    - Best trained model.\n",
    "    \"\"\"\n",
    "    # initialize model and optimizer and scheduler and criterion and best_val_loss and start_epoch\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Load checkpoint if resuming\n",
    "    if resume and os.path.exists(save_path):\n",
    "        try:\n",
    "            model, optimizer, scheduler, start_epoch, best_val_loss = load_checkpoint(\n",
    "                model, optimizer, scheduler, device, save_path\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "\n",
    "    no_improvement = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            train_loss, train_mae, train_rmse = training_loop(\n",
    "                model, train_loader, device, optimizer, criterion\n",
    "            )\n",
    "            val_loss, val_mae, val_rmse = validation_loop(\n",
    "                model, val_loader, device, criterion\n",
    "            )\n",
    "\n",
    "            # Scheduler step\n",
    "            if epoch > 0:\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "            # Print results\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}, Train RMSE: {train_rmse:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}, Val RMSE: {val_rmse:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Save best model checkpoint\n",
    "            if val_loss < best_val_loss:\n",
    "                print(\n",
    "                    f\"New best validation loss: {val_loss:.4f} (previous best: {best_val_loss:.4f})\"\n",
    "                )\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement = 0\n",
    "                save_checkpoint(\n",
    "                    epoch, model, optimizer, scheduler, best_val_loss, save_path\n",
    "                )\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                print(f\"No improvement, patience left: {patience - no_improvement}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if no_improvement >= patience:\n",
    "                print(f\"Early Stopping Triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training Interrupted! Saving last checkpoint...\")\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler, best_val_loss, save_path)\n",
    "\n",
    "    print(f\"Training Completed in {(time.time() - start_time):.2f} seconds\")\n",
    "\n",
    "    # Load the best model before returning\n",
    "    model, optimizer, scheduler, _, _ = load_checkpoint(\n",
    "        model, optimizer, scheduler, device, save_path\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = CpGCounter(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 1/50, Train Loss: 2.5070, Train MAE: 1.2232, Train RMSE: 1.5631, Val Loss: 1.1817, Val MAE: 0.8110, Val RMSE: 1.0319\n",
      "New best validation loss: 1.1817 (previous best: inf)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 2/50, Train Loss: 1.0280, Train MAE: 0.8017, Train RMSE: 0.9932, Val Loss: 0.8604, Val MAE: 0.7497, Val RMSE: 0.9089\n",
      "New best validation loss: 0.8604 (previous best: 1.1817)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 3/50, Train Loss: 0.7643, Train MAE: 0.6924, Train RMSE: 0.8549, Val Loss: 0.9482, Val MAE: 0.7549, Val RMSE: 0.9457\n",
      "No improvement, patience left: 4\n",
      "Epoch 4/50, Train Loss: 0.3489, Train MAE: 0.4511, Train RMSE: 0.5580, Val Loss: 0.1724, Val MAE: 0.2882, Val RMSE: 0.4067\n",
      "New best validation loss: 0.1724 (previous best: 0.8604)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 5/50, Train Loss: 0.1202, Train MAE: 0.2536, Train RMSE: 0.3324, Val Loss: 0.1413, Val MAE: 0.2668, Val RMSE: 0.3567\n",
      "New best validation loss: 0.1413 (previous best: 0.1724)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 6/50, Train Loss: 0.0714, Train MAE: 0.1896, Train RMSE: 0.2557, Val Loss: 0.0509, Val MAE: 0.1399, Val RMSE: 0.2062\n",
      "New best validation loss: 0.0509 (previous best: 0.1413)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 7/50, Train Loss: 0.0398, Train MAE: 0.1354, Train RMSE: 0.1886, Val Loss: 0.0542, Val MAE: 0.1249, Val RMSE: 0.1953\n",
      "No improvement, patience left: 4\n",
      "Epoch 8/50, Train Loss: 0.0391, Train MAE: 0.1329, Train RMSE: 0.1853, Val Loss: 0.0412, Val MAE: 0.1217, Val RMSE: 0.1828\n",
      "New best validation loss: 0.0412 (previous best: 0.0509)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 9/50, Train Loss: 0.0280, Train MAE: 0.1058, Train RMSE: 0.1534, Val Loss: 0.0220, Val MAE: 0.0862, Val RMSE: 0.1307\n",
      "New best validation loss: 0.0220 (previous best: 0.0412)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 10/50, Train Loss: 0.0209, Train MAE: 0.0920, Train RMSE: 0.1338, Val Loss: 0.0191, Val MAE: 0.0722, Val RMSE: 0.1164\n",
      "New best validation loss: 0.0191 (previous best: 0.0220)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Training Interrupted! Saving last checkpoint...\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Training Completed in 3.74 seconds\n",
      "Loaded checkpoint from best_cpg_model.pth, resuming from epoch 11 with best validation loss: 0.0191\n"
     ]
    }
   ],
   "source": [
    "best_model = train_model(\n",
    "    new_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    epochs=num_epochs,\n",
    "    patience=5,\n",
    "    save_path=\"best_cpg_model.pth\",\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.2883, device='cuda:0')\n",
      "tensor(-2.2883, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(best_model.state_dict()[\"embedding.weight\"].sum())\n",
    "print(new_model.state_dict()[\"embedding.weight\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cpgs_from_dna(\n",
    "    model_path: str,\n",
    "    dna_sequence: str,\n",
    "    dna2int: dict,\n",
    "    embedding_dim,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    device,\n",
    "    model_class=CpGCounter,\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict CpG count from a human DNA string.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path: Path to trained LSTM model.\n",
    "    - dna_sequence: Human-readable DNA string.\n",
    "    - dna2int: Dictionary mapping DNA bases to integer values.\n",
    "    - embedding_dim: Dimension of embedding layer.\n",
    "    - hidden_size: Size of LSTM hidden state.\n",
    "    - num_layers: Number of LSTM layers.\n",
    "    - dropout: Dropout rate.\n",
    "    - device: The device ('cpu' or 'cuda') for inference.\n",
    "\n",
    "    Returns:\n",
    "    - Predicted CpG count (rounded to 2 decimal places).\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the model checkpoint exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model checkpoint not found at {model_path}\")\n",
    "\n",
    "    # Load Model\n",
    "    vocab_size = len(dna2int)\n",
    "    model = model_class(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "\n",
    "    # Load the trained model checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)  # Move model to the correct device\n",
    "    model.eval()\n",
    "\n",
    "    # Convert DNA string to integer sequence\n",
    "    int_sequence = [\n",
    "        dna2int.get(base, 0) for base in dna_sequence\n",
    "    ]  # Map bases to integers\n",
    "    int_tensor = (\n",
    "        torch.tensor(int_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    )  # Move to same device\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        predicted_count = model(int_tensor).squeeze().item()  # Ensure it's a scalar\n",
    "\n",
    "    return round(predicted_count, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA: NCACANNTNCGGAGGCGNAGCTCG \n",
      "🔹 Predicted CpG Count: 3.24\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "test_dna = \"NCACANNTNCGGAGGCGNAGCTCG\"\n",
    "# no of CpG sites = 3\n",
    "\n",
    "\n",
    "predicted_cpgs = predict_cpgs_from_dna(\n",
    "    \"best_cpg_model.pth\",\n",
    "    test_dna,\n",
    "    dna2int,\n",
    "    embedding_dim,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    device,\n",
    "    model_class=CpGCounter,\n",
    ")\n",
    "\n",
    "print(f\"DNA: {test_dna} \\n🔹 Predicted CpG Count: {predicted_cpgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will have a improved version of the model additional elements like regularization, dropout, and batch normalization to improve the model's performance\n",
    "We will also use a more complex dataset with variable-length sequences to demonstrate the model's ability to handle such data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CpGCounterAdvanced(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(CpGCounterAdvanced, self).__init__()\n",
    "        # Embedding layer to convert integer-encoded sequences to embeddings of fixed size (embedding_dim)\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=0\n",
    "        )  # input: (batch_size, seq_len), output: (batch_size, seq_len, embedding_dim)\n",
    "        # LSTM layer to process the embeddings and capture sequential information (hidden_size) and bidirectional\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True,\n",
    "        )  # input: (batch_size, seq_len, embedding_dim), output: (batch_size, seq_len, hidden_size)\n",
    "        # add batch normalization 2 additional hidden states\n",
    "        self.batch_norm = nn.BatchNorm1d(\n",
    "            hidden_size * 2\n",
    "        )  # input: (batch_size, seq_len, hidden_size), output: (batch_size, seq_len, hidden_size)\n",
    "        # Fully connected layer to predict the number of CpG sites and ReLU activation\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_size * 2, 1\n",
    "        )  # input: (batch_size, seq_len, hidden_size), output: (batch_size, seq_len, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed the input sequences\n",
    "        embedded = self.embedding(x)\n",
    "        # Process the embeddings with an LSTM layer\n",
    "        lstm_output, _ = self.lstm(embedded)\n",
    "        # we only take the last hidden state\n",
    "        lstm_output = lstm_output[\n",
    "            :, -1, :\n",
    "        ]  # shape is (batch_size, hidden_size) and we only take the last hidden state of the sequence\n",
    "        # Batch normalization\n",
    "        lstm_output = self.batch_norm(lstm_output)\n",
    "        # Predict the number of CpG sites\n",
    "        output = self.fc(lstm_output)\n",
    "        return self.relu(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keep the validation loop unchanged\n",
    "- Use GradScaler only in the training loop \n",
    "- Use autocast() only in the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_advanced(\n",
    "    model, dataloader, device, optimizer, criterion, grad_scaler\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs one training epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): PyTorch model to train.\n",
    "    - dataloader (DataLoader): Training DataLoader.\n",
    "    - device (torch.device): CPU or GPU.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "\n",
    "    Returns:\n",
    "    - avg_loss (float): Average loss over dataset.\n",
    "    - avg_mae (float): Average MAE over dataset.\n",
    "    - avg_rmse (float): Average RMSE over dataset.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mae, total_rmse = 0.0, 0.0, 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = (\n",
    "            inputs.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(\"cuda\" if device.type == \"cuda\" else \"cpu\"):\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "\n",
    "        # Compute batch-wise metrics\n",
    "        batch_mae = mean_absolute_error(\n",
    "            labels.cpu().numpy(), outputs.detach().cpu().numpy()\n",
    "        )\n",
    "        batch_rmse = (\n",
    "            mean_squared_error(labels.cpu().numpy(), outputs.detach().cpu().numpy())\n",
    "            ** 0.5\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mae += batch_mae\n",
    "        total_rmse += batch_rmse\n",
    "\n",
    "    return total_loss / num_batches, total_mae / num_batches, total_rmse / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_advanced(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    epochs=25,\n",
    "    patience=5,\n",
    "    save_path=\"best_cpg_model_advanced.pth\",\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    resume=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains an LSTM model with validation and early stopping. Supports resuming training.\n",
    "\n",
    "    Parameters:\n",
    "    - model: LSTM model.\n",
    "    - train_loader: Training DataLoader.\n",
    "    - val_loader: Validation DataLoader.\n",
    "    - device: CPU or GPU.\n",
    "    - epochs: Max training epochs.\n",
    "    - patience: Early stopping patience.\n",
    "    - save_path: Path to save the best model.\n",
    "    - lr: Initial learning rate.\n",
    "    - weight_decay: L2 regularization weight.\n",
    "    - resume: Whether to resume training from the last checkpoint.\n",
    "\n",
    "    Returns:\n",
    "    - Best trained model.\n",
    "    \"\"\"\n",
    "    # initialize model and optimizer and scheduler and criterion and best_val_loss and start_epoch\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "    )\n",
    "    # initialize gradient scaler\n",
    "    grad_scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Load checkpoint if resuming\n",
    "    if resume and os.path.exists(save_path):\n",
    "        try:\n",
    "            model, optimizer, scheduler, start_epoch, best_val_loss = load_checkpoint(\n",
    "                model, optimizer, scheduler, device, save_path\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "\n",
    "    no_improvement = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            train_loss, train_mae, train_rmse = training_loop_advanced(\n",
    "                model, train_loader, device, optimizer, criterion, grad_scaler\n",
    "            )\n",
    "            val_loss, val_mae, val_rmse = validation_loop(\n",
    "                model, val_loader, device, criterion\n",
    "            )\n",
    "\n",
    "            # Scheduler step\n",
    "            if epoch > 0:\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "            # Print results\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}, Train RMSE: {train_rmse:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}, Val RMSE: {val_rmse:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Save best model checkpoint\n",
    "            if val_loss < best_val_loss:\n",
    "                print(\n",
    "                    f\"New best validation loss: {val_loss:.4f} (previous best: {best_val_loss:.4f})\"\n",
    "                )\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement = 0\n",
    "                save_checkpoint(\n",
    "                    epoch, model, optimizer, scheduler, best_val_loss, save_path\n",
    "                )\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                print(f\"No improvement, patience left: {patience - no_improvement}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if no_improvement >= patience:\n",
    "                print(f\"Early Stopping Triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training Interrupted! Saving last checkpoint...\")\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler, best_val_loss, save_path)\n",
    "\n",
    "    print(f\"Training Completed in {(time.time() - start_time):.2f} seconds\")\n",
    "\n",
    "    # Load the best model before returning\n",
    "    model, optimizer, scheduler, _, _ = load_checkpoint(\n",
    "        model, optimizer, scheduler, device, save_path\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CpGCounterAdvanced(\n",
      "  (embedding): Embedding(6, 32, padding_idx=0)\n",
      "  (lstm): LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize model and check the model architecture\n",
    "advanced_model = CpGCounterAdvanced(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ")\n",
    "print(advanced_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 1/50, Train Loss: 1.7186, Train MAE: 1.0320, Train RMSE: 1.2908, Val Loss: 1.5080, Val MAE: 0.8679, Val RMSE: 1.1319\n",
      "New best validation loss: 1.5080 (previous best: inf)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 2/50, Train Loss: 1.5898, Train MAE: 0.9833, Train RMSE: 1.2440, Val Loss: 1.1794, Val MAE: 0.7681, Val RMSE: 1.0123\n",
      "New best validation loss: 1.1794 (previous best: 1.5080)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 3/50, Train Loss: 1.3589, Train MAE: 0.9067, Train RMSE: 1.1408, Val Loss: 1.2141, Val MAE: 0.7913, Val RMSE: 1.0257\n",
      "No improvement, patience left: 4\n",
      "Epoch 4/50, Train Loss: 1.1039, Train MAE: 0.7801, Train RMSE: 1.0272, Val Loss: 0.8746, Val MAE: 0.7056, Val RMSE: 0.9086\n",
      "New best validation loss: 0.8746 (previous best: 1.1794)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 5/50, Train Loss: 0.6821, Train MAE: 0.5959, Train RMSE: 0.7945, Val Loss: 0.3252, Val MAE: 0.3981, Val RMSE: 0.5358\n",
      "New best validation loss: 0.3252 (previous best: 0.8746)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 6/50, Train Loss: 0.2978, Train MAE: 0.3844, Train RMSE: 0.5280, Val Loss: 0.2404, Val MAE: 0.2966, Val RMSE: 0.4424\n",
      "New best validation loss: 0.2404 (previous best: 0.3252)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 7/50, Train Loss: 0.2344, Train MAE: 0.3442, Train RMSE: 0.4560, Val Loss: 0.0401, Val MAE: 0.1315, Val RMSE: 0.1870\n",
      "New best validation loss: 0.0401 (previous best: 0.2404)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 8/50, Train Loss: 0.1730, Train MAE: 0.2980, Train RMSE: 0.3917, Val Loss: 0.0729, Val MAE: 0.1649, Val RMSE: 0.2538\n",
      "No improvement, patience left: 4\n",
      "Epoch 9/50, Train Loss: 0.1376, Train MAE: 0.2576, Train RMSE: 0.3413, Val Loss: 0.0842, Val MAE: 0.1973, Val RMSE: 0.2877\n",
      "No improvement, patience left: 3\n",
      "Epoch 10/50, Train Loss: 0.1299, Train MAE: 0.2562, Train RMSE: 0.3361, Val Loss: 0.0926, Val MAE: 0.2187, Val RMSE: 0.3011\n",
      "No improvement, patience left: 2\n",
      "Epoch 11/50, Train Loss: 0.1061, Train MAE: 0.2301, Train RMSE: 0.2973, Val Loss: 0.0180, Val MAE: 0.0899, Val RMSE: 0.1291\n",
      "New best validation loss: 0.0180 (previous best: 0.0401)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 12/50, Train Loss: 0.0778, Train MAE: 0.1950, Train RMSE: 0.2533, Val Loss: 0.0217, Val MAE: 0.0966, Val RMSE: 0.1423\n",
      "No improvement, patience left: 4\n",
      "Epoch 13/50, Train Loss: 0.0847, Train MAE: 0.2047, Train RMSE: 0.2594, Val Loss: 0.0149, Val MAE: 0.0836, Val RMSE: 0.1182\n",
      "New best validation loss: 0.0149 (previous best: 0.0180)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 14/50, Train Loss: 0.0969, Train MAE: 0.2132, Train RMSE: 0.2648, Val Loss: 0.0206, Val MAE: 0.0992, Val RMSE: 0.1370\n",
      "No improvement, patience left: 4\n",
      "Epoch 15/50, Train Loss: 0.0761, Train MAE: 0.1929, Train RMSE: 0.2427, Val Loss: 0.0237, Val MAE: 0.0892, Val RMSE: 0.1347\n",
      "No improvement, patience left: 3\n",
      "Epoch 16/50, Train Loss: 0.0953, Train MAE: 0.2133, Train RMSE: 0.2580, Val Loss: 0.0171, Val MAE: 0.0760, Val RMSE: 0.1198\n",
      "No improvement, patience left: 2\n",
      "Epoch 17/50, Train Loss: 0.0905, Train MAE: 0.2129, Train RMSE: 0.2646, Val Loss: 0.0204, Val MAE: 0.0847, Val RMSE: 0.1275\n",
      "No improvement, patience left: 1\n",
      "Epoch 18/50, Train Loss: 0.0730, Train MAE: 0.1917, Train RMSE: 0.2442, Val Loss: 0.0320, Val MAE: 0.1141, Val RMSE: 0.1600\n",
      "No improvement, patience left: 0\n",
      "Early Stopping Triggered after epoch 18\n",
      "Training Completed in 8.28 seconds\n",
      "Loaded checkpoint from best_cpg_model_advanced.pth, resuming from epoch 13 with best validation loss: 0.0149\n"
     ]
    }
   ],
   "source": [
    "best_model_advanced = train_model_advanced(\n",
    "    advanced_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    epochs=num_epochs,\n",
    "    patience=5,\n",
    "    save_path=\"best_cpg_model_advanced.pth\",\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA: NCAACGCGNAGCTCGGCNAGCTCG \n",
      "🔹 Predicted CpG Count: 4.46\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "test_dna = \"NCAACGCGNAGCTCGGCNAGCTCG\"\n",
    "# no of cpg sites in the test example is 4\n",
    "\n",
    "\n",
    "predicted_cpgs = predict_cpgs_from_dna(\n",
    "    \"best_cpg_model_advanced.pth\",\n",
    "    test_dna,\n",
    "    dna2int,\n",
    "    embedding_dim,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    device,\n",
    "    model_class=CpGCounterAdvanced,\n",
    ")\n",
    "\n",
    "print(f\"DNA: {test_dna} \\n🔹 Predicted CpG Count: {predicted_cpgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we will train the model on variable length sequences with only padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------ Data Preparation ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 100\n",
      "900 100\n"
     ]
    }
   ],
   "source": [
    "# Split variable-length dataset (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_variable, y_variable, test_size=0.1, random_state=13\n",
    ")\n",
    "print(len(X_train), len(X_val))\n",
    "print(len(y_train), len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 100\n",
      "16 32\n"
     ]
    }
   ],
   "source": [
    "# pytorch standard dataset\n",
    "train_dataset = CPGDataset(X_train, y_train)\n",
    "val_dataset = CPGDataset(X_val, y_val)\n",
    "\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "# check different lengths of sequences\n",
    "print(min(map(len, X_train)), max(map(len, X_train)))\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=CPGDataset.collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=CPGDataset.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 31]) torch.Size([16])\n",
      "torch.Size([16, 32]) torch.Size([16])\n",
      "torch.Size([16, 32]) torch.Size([16])\n",
      "torch.Size([16, 32]) torch.Size([16])\n",
      "torch.Size([16, 31]) torch.Size([16])\n",
      "----\n",
      "torch.Size([16, 32]) torch.Size([16])\n",
      "torch.Size([16, 30]) torch.Size([16])\n",
      "torch.Size([16, 32]) torch.Size([16])\n",
      "torch.Size([16, 31]) torch.Size([16])\n",
      "torch.Size([16, 32]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x_batch, y_batch in train_dataloader:\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n",
    "print(\"----\")\n",
    "i = 0\n",
    "for x_batch, y_batch in val_dataloader:\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Model Training with Variable-Length Sequences (LSTM) and Advanced Model Training with Variable-Length Sequences (LSTM + BatchNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = CpGCounter(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 1/50, Train Loss: 0.9302, Train MAE: 0.7714, Train RMSE: 0.9495, Val Loss: 0.7795, Val MAE: 0.6885, Val RMSE: 0.8534\n",
      "New best validation loss: 0.7795 (previous best: inf)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 2/50, Train Loss: 0.7226, Train MAE: 0.7017, Train RMSE: 0.8340, Val Loss: 0.4869, Val MAE: 0.5521, Val RMSE: 0.6775\n",
      "New best validation loss: 0.4869 (previous best: 0.7795)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 3/50, Train Loss: 0.4878, Train MAE: 0.5664, Train RMSE: 0.6800, Val Loss: 0.3238, Val MAE: 0.4529, Val RMSE: 0.5483\n",
      "New best validation loss: 0.3238 (previous best: 0.4869)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 4/50, Train Loss: 0.2458, Train MAE: 0.3766, Train RMSE: 0.4785, Val Loss: 0.1645, Val MAE: 0.2930, Val RMSE: 0.3935\n",
      "New best validation loss: 0.1645 (previous best: 0.3238)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 5/50, Train Loss: 0.1069, Train MAE: 0.2023, Train RMSE: 0.2978, Val Loss: 0.0732, Val MAE: 0.1695, Val RMSE: 0.2590\n",
      "New best validation loss: 0.0732 (previous best: 0.1645)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 6/50, Train Loss: 0.0320, Train MAE: 0.0943, Train RMSE: 0.1626, Val Loss: 0.0296, Val MAE: 0.0898, Val RMSE: 0.1643\n",
      "New best validation loss: 0.0296 (previous best: 0.0732)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 7/50, Train Loss: 0.0447, Train MAE: 0.1226, Train RMSE: 0.1942, Val Loss: 0.0395, Val MAE: 0.0949, Val RMSE: 0.1784\n",
      "No improvement, patience left: 9\n",
      "Epoch 8/50, Train Loss: 0.0228, Train MAE: 0.0777, Train RMSE: 0.1355, Val Loss: 0.0173, Val MAE: 0.0707, Val RMSE: 0.1260\n",
      "New best validation loss: 0.0173 (previous best: 0.0296)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 9/50, Train Loss: 0.0189, Train MAE: 0.0737, Train RMSE: 0.1288, Val Loss: 0.0233, Val MAE: 0.0755, Val RMSE: 0.1417\n",
      "No improvement, patience left: 9\n",
      "Epoch 10/50, Train Loss: 0.0176, Train MAE: 0.0748, Train RMSE: 0.1232, Val Loss: 0.0147, Val MAE: 0.0563, Val RMSE: 0.1116\n",
      "New best validation loss: 0.0147 (previous best: 0.0173)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 11/50, Train Loss: 0.0145, Train MAE: 0.0641, Train RMSE: 0.1082, Val Loss: 0.0098, Val MAE: 0.0501, Val RMSE: 0.0903\n",
      "New best validation loss: 0.0098 (previous best: 0.0147)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 12/50, Train Loss: 0.0088, Train MAE: 0.0499, Train RMSE: 0.0870, Val Loss: 0.0163, Val MAE: 0.0556, Val RMSE: 0.1107\n",
      "No improvement, patience left: 9\n",
      "Epoch 13/50, Train Loss: 0.0124, Train MAE: 0.0590, Train RMSE: 0.1019, Val Loss: 0.0118, Val MAE: 0.0603, Val RMSE: 0.1056\n",
      "No improvement, patience left: 8\n",
      "Epoch 14/50, Train Loss: 0.0137, Train MAE: 0.0645, Train RMSE: 0.1068, Val Loss: 0.0138, Val MAE: 0.0487, Val RMSE: 0.0956\n",
      "No improvement, patience left: 7\n",
      "Epoch 15/50, Train Loss: 0.0050, Train MAE: 0.0335, Train RMSE: 0.0621, Val Loss: 0.0054, Val MAE: 0.0342, Val RMSE: 0.0677\n",
      "New best validation loss: 0.0054 (previous best: 0.0098)\n",
      "Model checkpoint saved at best_cpg_model.pth\n",
      "Epoch 16/50, Train Loss: 0.0038, Train MAE: 0.0290, Train RMSE: 0.0550, Val Loss: 0.0062, Val MAE: 0.0357, Val RMSE: 0.0697\n",
      "No improvement, patience left: 9\n",
      "Epoch 17/50, Train Loss: 0.0038, Train MAE: 0.0290, Train RMSE: 0.0523, Val Loss: 0.0058, Val MAE: 0.0393, Val RMSE: 0.0714\n",
      "No improvement, patience left: 8\n",
      "Epoch 18/50, Train Loss: 0.0033, Train MAE: 0.0283, Train RMSE: 0.0521, Val Loss: 0.0079, Val MAE: 0.0383, Val RMSE: 0.0769\n",
      "No improvement, patience left: 7\n",
      "Epoch 19/50, Train Loss: 0.0031, Train MAE: 0.0272, Train RMSE: 0.0500, Val Loss: 0.0057, Val MAE: 0.0352, Val RMSE: 0.0671\n",
      "No improvement, patience left: 6\n",
      "Epoch 20/50, Train Loss: 0.0037, Train MAE: 0.0267, Train RMSE: 0.0481, Val Loss: 0.0055, Val MAE: 0.0349, Val RMSE: 0.0662\n",
      "No improvement, patience left: 5\n",
      "Epoch 21/50, Train Loss: 0.0027, Train MAE: 0.0256, Train RMSE: 0.0466, Val Loss: 0.0055, Val MAE: 0.0346, Val RMSE: 0.0658\n",
      "No improvement, patience left: 4\n",
      "Epoch 22/50, Train Loss: 0.0028, Train MAE: 0.0257, Train RMSE: 0.0476, Val Loss: 0.0055, Val MAE: 0.0344, Val RMSE: 0.0657\n",
      "No improvement, patience left: 3\n",
      "Epoch 23/50, Train Loss: 0.0027, Train MAE: 0.0260, Train RMSE: 0.0465, Val Loss: 0.0055, Val MAE: 0.0343, Val RMSE: 0.0656\n",
      "No improvement, patience left: 2\n",
      "Epoch 24/50, Train Loss: 0.0027, Train MAE: 0.0251, Train RMSE: 0.0451, Val Loss: 0.0055, Val MAE: 0.0344, Val RMSE: 0.0656\n",
      "No improvement, patience left: 1\n",
      "Epoch 25/50, Train Loss: 0.0030, Train MAE: 0.0254, Train RMSE: 0.0465, Val Loss: 0.0055, Val MAE: 0.0344, Val RMSE: 0.0656\n",
      "No improvement, patience left: 0\n",
      "Early Stopping Triggered after epoch 25\n",
      "Training Completed in 5.85 seconds\n",
      "Loaded checkpoint from best_cpg_model.pth, resuming from epoch 15 with best validation loss: 0.0054\n"
     ]
    }
   ],
   "source": [
    "best_model = train_model(\n",
    "    new_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    epochs=num_epochs,\n",
    "    patience=10,\n",
    "    save_path=\"best_cpg_model.pth\",\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA: NCACANNTNCGGAGGCGNAGCTCG \n",
      "🔹 Predicted CpG Count: 3.01\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "test_dna = \"NCACANNTNCGGAGGCGNAGCTCG\"\n",
    "# no of CpG sites = 3\n",
    "\n",
    "predicted_cpgs = predict_cpgs_from_dna(\n",
    "    \"best_cpg_model.pth\",\n",
    "    test_dna,\n",
    "    dna2int,\n",
    "    embedding_dim,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    device,\n",
    "    model_class=CpGCounter,\n",
    ")\n",
    "\n",
    "print(f\"DNA: {test_dna} \\n🔹 Predicted CpG Count: {predicted_cpgs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CpGCounterAdvanced(\n",
      "  (embedding): Embedding(6, 32, padding_idx=0)\n",
      "  (lstm): LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize model and check the model architecture\n",
    "advanced_model = CpGCounterAdvanced(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ")\n",
    "print(advanced_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 1/50, Train Loss: 1.2382, Train MAE: 0.8367, Train RMSE: 1.0873, Val Loss: 1.1232, Val MAE: 0.7975, Val RMSE: 1.0384\n",
      "New best validation loss: 1.1232 (previous best: inf)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 2/50, Train Loss: 1.0024, Train MAE: 0.7535, Train RMSE: 0.9696, Val Loss: 0.7408, Val MAE: 0.6321, Val RMSE: 0.8458\n",
      "New best validation loss: 0.7408 (previous best: 1.1232)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 3/50, Train Loss: 0.7406, Train MAE: 0.6242, Train RMSE: 0.8424, Val Loss: 0.4314, Val MAE: 0.4677, Val RMSE: 0.6521\n",
      "New best validation loss: 0.4314 (previous best: 0.7408)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 4/50, Train Loss: 0.5260, Train MAE: 0.5069, Train RMSE: 0.7053, Val Loss: 0.4188, Val MAE: 0.4911, Val RMSE: 0.6347\n",
      "New best validation loss: 0.4188 (previous best: 0.4314)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 5/50, Train Loss: 0.3015, Train MAE: 0.3513, Train RMSE: 0.5250, Val Loss: 0.1618, Val MAE: 0.2473, Val RMSE: 0.3882\n",
      "New best validation loss: 0.1618 (previous best: 0.4188)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 6/50, Train Loss: 0.1738, Train MAE: 0.2521, Train RMSE: 0.3939, Val Loss: 0.2131, Val MAE: 0.2703, Val RMSE: 0.4485\n",
      "No improvement, patience left: 4\n",
      "Epoch 7/50, Train Loss: 0.1850, Train MAE: 0.2669, Train RMSE: 0.4018, Val Loss: 0.1247, Val MAE: 0.2162, Val RMSE: 0.3473\n",
      "New best validation loss: 0.1247 (previous best: 0.1618)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 8/50, Train Loss: 0.1006, Train MAE: 0.1915, Train RMSE: 0.2915, Val Loss: 0.1299, Val MAE: 0.2045, Val RMSE: 0.3436\n",
      "No improvement, patience left: 4\n",
      "Epoch 9/50, Train Loss: 0.1347, Train MAE: 0.2297, Train RMSE: 0.3380, Val Loss: 0.0683, Val MAE: 0.2214, Val RMSE: 0.2606\n",
      "New best validation loss: 0.0683 (previous best: 0.1247)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 10/50, Train Loss: 0.1260, Train MAE: 0.2162, Train RMSE: 0.3222, Val Loss: 0.0453, Val MAE: 0.1331, Val RMSE: 0.2089\n",
      "New best validation loss: 0.0453 (previous best: 0.0683)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 11/50, Train Loss: 0.0889, Train MAE: 0.1798, Train RMSE: 0.2638, Val Loss: 0.0318, Val MAE: 0.1007, Val RMSE: 0.1737\n",
      "New best validation loss: 0.0318 (previous best: 0.0453)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 12/50, Train Loss: 0.0809, Train MAE: 0.1795, Train RMSE: 0.2548, Val Loss: 0.1818, Val MAE: 0.3044, Val RMSE: 0.4178\n",
      "No improvement, patience left: 4\n",
      "Epoch 13/50, Train Loss: 0.0693, Train MAE: 0.1608, Train RMSE: 0.2394, Val Loss: 0.3751, Val MAE: 0.4372, Val RMSE: 0.6050\n",
      "No improvement, patience left: 3\n",
      "Epoch 14/50, Train Loss: 0.0860, Train MAE: 0.1823, Train RMSE: 0.2643, Val Loss: 0.2116, Val MAE: 0.3147, Val RMSE: 0.4536\n",
      "No improvement, patience left: 2\n",
      "Epoch 15/50, Train Loss: 0.0737, Train MAE: 0.1647, Train RMSE: 0.2365, Val Loss: 0.0111, Val MAE: 0.0560, Val RMSE: 0.0985\n",
      "New best validation loss: 0.0111 (previous best: 0.0318)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 16/50, Train Loss: 0.0539, Train MAE: 0.1409, Train RMSE: 0.1996, Val Loss: 0.0111, Val MAE: 0.0555, Val RMSE: 0.0985\n",
      "New best validation loss: 0.0111 (previous best: 0.0111)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 17/50, Train Loss: 0.0719, Train MAE: 0.1749, Train RMSE: 0.2383, Val Loss: 0.0091, Val MAE: 0.0579, Val RMSE: 0.0920\n",
      "New best validation loss: 0.0091 (previous best: 0.0111)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 18/50, Train Loss: 0.0581, Train MAE: 0.1424, Train RMSE: 0.1946, Val Loss: 0.0168, Val MAE: 0.0680, Val RMSE: 0.1227\n",
      "No improvement, patience left: 4\n",
      "Epoch 19/50, Train Loss: 0.0559, Train MAE: 0.1407, Train RMSE: 0.1944, Val Loss: 0.0075, Val MAE: 0.0483, Val RMSE: 0.0803\n",
      "New best validation loss: 0.0075 (previous best: 0.0091)\n",
      "Model checkpoint saved at best_cpg_model_advanced.pth\n",
      "Epoch 20/50, Train Loss: 0.0585, Train MAE: 0.1462, Train RMSE: 0.2034, Val Loss: 0.0181, Val MAE: 0.0757, Val RMSE: 0.1266\n",
      "No improvement, patience left: 4\n",
      "Epoch 21/50, Train Loss: 0.0596, Train MAE: 0.1533, Train RMSE: 0.2105, Val Loss: 0.0096, Val MAE: 0.0623, Val RMSE: 0.0930\n",
      "No improvement, patience left: 3\n",
      "Epoch 22/50, Train Loss: 0.0578, Train MAE: 0.1389, Train RMSE: 0.1894, Val Loss: 0.0091, Val MAE: 0.0534, Val RMSE: 0.0924\n",
      "No improvement, patience left: 2\n",
      "Epoch 23/50, Train Loss: 0.0655, Train MAE: 0.1609, Train RMSE: 0.2081, Val Loss: 0.0179, Val MAE: 0.0926, Val RMSE: 0.1306\n",
      "No improvement, patience left: 1\n",
      "Epoch 24/50, Train Loss: 0.0553, Train MAE: 0.1560, Train RMSE: 0.2033, Val Loss: 0.0076, Val MAE: 0.0532, Val RMSE: 0.0827\n",
      "No improvement, patience left: 0\n",
      "Early Stopping Triggered after epoch 24\n",
      "Training Completed in 10.91 seconds\n",
      "Loaded checkpoint from best_cpg_model_advanced.pth, resuming from epoch 19 with best validation loss: 0.0075\n"
     ]
    }
   ],
   "source": [
    "best_model_advanced = train_model_advanced(\n",
    "    advanced_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    epochs=num_epochs,\n",
    "    patience=5,\n",
    "    save_path=\"best_cpg_model_advanced.pth\",\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA: NCAACGCGNAGCTCGGCNAGCTCG \n",
      "🔹 Predicted CpG Count: 4.1\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "test_dna = \"NCAACGCGNAGCTCGGCNAGCTCG\"\n",
    "# no of cpg sites in the test example is 4\n",
    "\n",
    "\n",
    "predicted_cpgs = predict_cpgs_from_dna(\n",
    "    \"best_cpg_model_advanced.pth\",\n",
    "    test_dna,\n",
    "    dna2int,\n",
    "    embedding_dim,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    device,\n",
    "    model_class=CpGCounterAdvanced,\n",
    ")\n",
    "\n",
    "print(f\"DNA: {test_dna} \\n🔹 Predicted CpG Count: {predicted_cpgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we will try using packed sequences to train the model with variable length sequences which will be more efficient. The problems with only padding are:\n",
    "- The model processes padded values, which are meaningless and waste computation.\n",
    "- The model treats all sequences as the same length, which is not true for variable-length sequences.\n",
    "- The model may not learn the actual sequence patterns due to the presence of padding.\n",
    "##### Packed sequences solve these problems by:\n",
    "- Ignoring padded values during computation.\n",
    "- Preserving the original sequence lengths for each batch.\n",
    "- Allowing the model to learn the actual sequence patterns without interference from padding.\n",
    "- Improving the model's efficiency and accuracy.\n",
    "- We will use the pack_padded_sequence and pad_packed_sequence functions in PyTorch to handle variable-length sequences.\n",
    "\n",
    "##### Padding and Packing Sequences for RNNs in PyTorch\n",
    "\n",
    "- Sort sequences by length (longest first).\n",
    "- Pack sequences using torch.nn.utils.rnn.pack_padded_sequence.\n",
    "- LSTM skips padded timesteps, processing only actual data.\n",
    "- Unpack the sequences back using pad_packed_sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPGDatasetPackPadding(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with sequences and labels (CpG counts) as input arguments and store them as attributes.\n",
    "\n",
    "        Parameters:\n",
    "        - sequences (List[List[int]]): List of integer-encoded DNA sequences.\n",
    "        - labels (List[int]): List of CpG counts corresponding to each sequence.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        self.sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a sequence and its label by index.\n",
    "        \"\"\"\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to pack sequences dynamically and return a batch of sequences and labels.\n",
    "        \"\"\"\n",
    "        sequences, labels = zip(*batch)\n",
    "        # find the lenghts of the sequences and sort them\n",
    "        lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "        sorted_indices = torch.argsort(lengths, descending=True)\n",
    "        # sort sequences by length\n",
    "        sequences = [sequences[i] for i in sorted_indices]\n",
    "        # sort labels by length\n",
    "        labels = torch.tensor([labels[i] for i in sorted_indices], dtype=torch.float32)\n",
    "        lengths = lengths[sorted_indices]\n",
    "\n",
    "        # pad sequences and return a packed sequence and labels and lengths\n",
    "        pad_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "\n",
    "        return pad_sequences, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 100\n"
     ]
    }
   ],
   "source": [
    "# Split the X_variable and y_variable\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_variable, y_variable, test_size=0.1, random_state=18\n",
    ")\n",
    "print(len(X_train), len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch standard dataset\n",
    "train_dataset = CPGDatasetPackPadding(X_train, y_train)\n",
    "val_dataset = CPGDatasetPackPadding(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 32\n"
     ]
    }
   ],
   "source": [
    "# check different lengths of sequences\n",
    "print(min(map(len, X_train)), max(map(len, X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader with pack padding sequence\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=CPGDatasetPackPadding.collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=CPGDatasetPackPadding.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32]) torch.Size([16]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch, lengths in train_dataloader:\n",
    "    print(x_batch.shape, y_batch.shape, lengths.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CpGCounterAdvancedPackPadding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(CpGCounterAdvancedPackPadding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Forward pass using packed sequences.\n",
    "\n",
    "        Parameters:\n",
    "        - x: (batch_size, seq_len) -> Padded sequences.\n",
    "        - lengths: (batch_size) -> Actual sequence lengths.\n",
    "\n",
    "        Returns:\n",
    "        - CpG count prediction.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(\n",
    "            x\n",
    "        )  # input: (batch_size, seq_len), output: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Pack sequence to ignore padding\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "\n",
    "        # LSTM processing\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "\n",
    "        # Unpack sequence to get the output of each time step (hidden states)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # Extract last valid hidden state dynamically (from both directions)\n",
    "        batch_indices = torch.arange(x.size(0), device=x.device)\n",
    "        last_indices = lengths - 1\n",
    "        last_hidden_states = output[batch_indices, last_indices, :]\n",
    "\n",
    "        # Apply batch norm & fully connected layer\n",
    "        last_hidden_states = self.batch_norm(last_hidden_states)\n",
    "        output = self.fc(last_hidden_states)\n",
    "        return self.relu(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **2️⃣ Shape Transformations at Each Step**\n",
    "Let's assume:\n",
    "- `batch_size = 4`\n",
    "- `max_seq_len = 6`\n",
    "- `embedding_dim = 64`\n",
    "- `hidden_size = 128`\n",
    "- `bidirectional = True` (so `hidden_size * 2` is the actual hidden size)\n",
    "- Example sequences with variable lengths:\n",
    "  ```\n",
    "  Input Sequences (Padded) → Shape: (4, 6)\n",
    "  ┌────────────┬───────────┐\n",
    "  │   Seq      │  Length   │\n",
    "  ├────────────┼───────────┤\n",
    "  │ [A, C, G]  │ 3         │\n",
    "  │ [T, A, C, G] │ 4      │\n",
    "  │ [G, C, T, A, G, C] │ 6  │\n",
    "  │ [A]        │ 1         │\n",
    "  └────────────┴───────────┘\n",
    "  ```\n",
    "\n",
    "### **🔹 Step 1: Embedding Layer**\n",
    "```python\n",
    "embedded = self.embedding(x)\n",
    "```\n",
    "🔹 **Before**: `x` → **(batch_size, seq_len) = (4, 6)** (Integer indices of DNA bases)  \n",
    "🔹 **After**: `embedded` → **(batch_size, seq_len, embedding_dim) = (4, 6, 64)**  \n",
    "- Each nucleotide is converted into a **64-dimensional embedding vector**.\n",
    "\n",
    "**Example Output Shape**:\n",
    "```\n",
    "embedded.shape = (4, 6, 64)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Step 2: Pack the Embedded Sequences**\n",
    "```python\n",
    "packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "```\n",
    "🔹 **Before**: `embedded.shape = (4, 6, 64)`  \n",
    "🔹 **After**: `packed_embedded.shape ≠ (4, 6, 64)` (Packed representation removes padding!)\n",
    "\n",
    "**Key Points:**\n",
    "- **Pack sequences to remove padding** before sending them into LSTM.\n",
    "- The **LSTM now only processes valid sequence elements**, ignoring padding.\n",
    "- **No fixed shape** because it's a packed structure.\n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Step 3: LSTM Processing**\n",
    "```python\n",
    "packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "```\n",
    "🔹 **Before**: `packed_embedded` (Packed sequence object)  \n",
    "🔹 **After**: \n",
    "- `packed_output`: Packed sequence object with LSTM outputs  \n",
    "- `hidden`: `(num_layers * 2, batch_size, hidden_size)`  \n",
    "- `cell`: `(num_layers * 2, batch_size, hidden_size)`\n",
    "\n",
    "#### **Shapes Explanation**\n",
    "- **Hidden State (`hidden`)**:\n",
    "  ```\n",
    "  Shape: (num_layers * 2, batch_size, hidden_size)\n",
    "  Example: (2, 4, 128) if num_layers=1, bidirectional=True\n",
    "  ```\n",
    "- **Packed Output (`packed_output`)**:\n",
    "  - Cannot have a fixed shape (depends on `pack_padded_sequence()`).\n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Step 4: Unpack the Sequences**\n",
    "```python\n",
    "output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "```\n",
    "🔹 **Before**: `packed_output` (Packed sequence)  \n",
    "🔹 **After**: `output.shape = (batch_size, seq_len, hidden_size * 2) = (4, 6, 256)`\n",
    "\n",
    "**Key Transformation**:\n",
    "- Converts **packed LSTM output** back to a **padded format**.\n",
    "- Now we have a **fixed shape**, but the padding has no meaningful values.\n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Step 5: Extract Last Valid Hidden State**\n",
    "```python\n",
    "batch_indices = torch.arange(x.size(0), device=x.device)\n",
    "last_indices = lengths - 1\n",
    "last_hidden_states = output[batch_indices, last_indices, :]\n",
    "```\n",
    "🔹 **Before**: `output.shape = (4, 6, 256)`  \n",
    "🔹 **After**: `last_hidden_states.shape = (4, 256)`\n",
    "\n",
    "**Why is this needed?**\n",
    "- Since sequences have **different lengths**, the last meaningful hidden state **is not always at `[:, -1, :]`**.\n",
    "- Instead, we **extract the last valid hidden state per sequence** using `lengths`.\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Lengths = [3, 4, 6, 1] (Actual sequence lengths)\n",
    "\n",
    "Indices:\n",
    "  Sequence 1 → last valid state = output[0, 2, :]  (index 2)\n",
    "  Sequence 2 → last valid state = output[1, 3, :]  (index 3)\n",
    "  Sequence 3 → last valid state = output[2, 5, :]  (index 5)\n",
    "  Sequence 4 → last valid state = output[3, 0, :]  (index 0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Step 6: Apply Batch Normalization & Fully Connected Layer**\n",
    "```python\n",
    "last_hidden_states = self.batch_norm(last_hidden_states)\n",
    "output = self.fc(last_hidden_states)\n",
    "return self.relu(output)\n",
    "```\n",
    "🔹 **Before**: `last_hidden_states.shape = (4, 256)`  \n",
    "🔹 **After Batch Norm**: **`(4, 256)`** (Batch normalization applied)  \n",
    "🔹 **After Fully Connected Layer**: **`(4, 1)`**  \n",
    "🔹 **After ReLU Activation**: **`(4, 1)`**  \n",
    "\n",
    "---\n",
    "## **📌 Summary of Shape Transformations**\n",
    "| **Step** | **Operation** | **Input Shape** | **Output Shape** |\n",
    "|---------|-------------|--------------|--------------|\n",
    "| **1️⃣** | Embedding Layer | `(batch_size, seq_len)` → `(4, 6)` | `(batch_size, seq_len, embedding_dim)` → `(4, 6, 64)` |\n",
    "| **2️⃣** | Pack Sequences | `(4, 6, 64)` | Packed Sequence Object (removes padding) |\n",
    "| **3️⃣** | LSTM Processing | Packed Sequence | `hidden: (num_layers*2, batch_size, hidden_size)` |\n",
    "| **4️⃣** | Unpack Sequences | Packed Output | `(batch_size, seq_len, hidden_size * 2)` → `(4, 6, 256)` |\n",
    "| **5️⃣** | Extract Last Valid State | `(4, 6, 256)` | `(batch_size, hidden_size * 2)` → `(4, 256)` |\n",
    "| **6️⃣** | Batch Norm + Fully Connected | `(4, 256)` | `(4, 1)` |\n",
    "\n",
    "---\n",
    "\n",
    "## **🚀 Key Takeaways**\n",
    "✅ **Using `pack_padded_sequence()` improves efficiency**  \n",
    "✅ **Unpacking and extracting last valid state avoids padding bias**  \n",
    "✅ **Dynamic last hidden state selection improves performance**  \n",
    "✅ **Packed sequences reduce unnecessary computation on padded tokens**  \n",
    "\n",
    "🚀 **Now your model is optimized for CpG site counting!** 🔥 Let me know if you have any doubts! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_pack_padded(\n",
    "    model, dataloader, device, optimizer, criterion, grad_scaler\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs one training epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): PyTorch model to train.\n",
    "    - dataloader (DataLoader): Training DataLoader.\n",
    "    - device (torch.device): CPU or GPU.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "\n",
    "    Returns:\n",
    "    - avg_loss (float): Average loss over dataset.\n",
    "    - avg_mae (float): Average MAE over dataset.\n",
    "    - avg_rmse (float): Average RMSE over dataset.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mae, total_rmse = 0.0, 0.0, 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for inputs, labels, lengths in dataloader:\n",
    "        inputs, labels, lengths = (\n",
    "            inputs.to(device),\n",
    "            labels.to(device),\n",
    "            lengths.to(device),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(\"cuda\" if device.type == \"cuda\" else \"cpu\"):\n",
    "            outputs = model(inputs, lengths).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "\n",
    "        # Compute batch-wise metrics\n",
    "        batch_mae = mean_absolute_error(\n",
    "            labels.cpu().numpy(), outputs.detach().cpu().numpy()\n",
    "        )\n",
    "        batch_rmse = (\n",
    "            mean_squared_error(labels.cpu().numpy(), outputs.detach().cpu().numpy())\n",
    "            ** 0.5\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mae += batch_mae\n",
    "        total_rmse += batch_rmse\n",
    "\n",
    "    return total_loss / num_batches, total_mae / num_batches, total_rmse / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop_pack_padded(model, dataloader, device, criterion):\n",
    "    \"\"\"\n",
    "    Runs one validation epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): PyTorch model to evaluate.\n",
    "    - dataloader (DataLoader): Validation DataLoader.\n",
    "    - device (torch.device): CPU or GPU.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "\n",
    "    Returns:\n",
    "    - avg_loss (float): Average loss over dataset.\n",
    "    - avg_mae (float): Average MAE over dataset.\n",
    "    - avg_rmse (float): Average RMSE over dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_mae, total_rmse = 0.0, 0.0, 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, lengths in dataloader:\n",
    "            inputs, labels, lengths = (\n",
    "                inputs.to(device),\n",
    "                labels.to(device),\n",
    "                lengths.to(device),\n",
    "            )\n",
    "            outputs = model(inputs, lengths).squeeze()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute batch-wise metrics\n",
    "            batch_mae = mean_absolute_error(labels.cpu().numpy(), outputs.cpu().numpy())\n",
    "            batch_rmse = (\n",
    "                mean_squared_error(labels.cpu().numpy(), outputs.cpu().numpy()) ** 0.5\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae += batch_mae\n",
    "            total_rmse += batch_rmse\n",
    "\n",
    "    return total_loss / num_batches, total_mae / num_batches, total_rmse / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pack_padded(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    epochs=25,\n",
    "    patience=5,\n",
    "    save_path=\"best_cpg_model_advanced.pth\",\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    resume=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains an LSTM model with validation and early stopping. Supports resuming training.\n",
    "\n",
    "    Parameters:\n",
    "    - model: LSTM model.\n",
    "    - train_loader: Training DataLoader.\n",
    "    - val_loader: Validation DataLoader.\n",
    "    - device: CPU or GPU.\n",
    "    - epochs: Max training epochs.\n",
    "    - patience: Early stopping patience.\n",
    "    - save_path: Path to save the best model.\n",
    "    - lr: Initial learning rate.\n",
    "    - weight_decay: L2 regularization weight.\n",
    "    - resume: Whether to resume training from the last checkpoint.\n",
    "\n",
    "    Returns:\n",
    "    - Best trained model.\n",
    "    \"\"\"\n",
    "    # initialize model and optimizer and scheduler and criterion and best_val_loss and start_epoch\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "    )\n",
    "    # initialize gradient scaler\n",
    "    grad_scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Load checkpoint if resuming\n",
    "    if resume and os.path.exists(save_path):\n",
    "        try:\n",
    "            model, optimizer, scheduler, start_epoch, best_val_loss = load_checkpoint(\n",
    "                model, optimizer, scheduler, device, save_path\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "\n",
    "    no_improvement = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            train_loss, train_mae, train_rmse = training_loop_pack_padded(\n",
    "                model, train_loader, device, optimizer, criterion, grad_scaler\n",
    "            )\n",
    "            val_loss, val_mae, val_rmse = validation_loop_pack_padded(\n",
    "                model, val_loader, device, criterion\n",
    "            )\n",
    "\n",
    "            # Scheduler step\n",
    "            if epoch > 0:\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "            # Print results\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}, Train RMSE: {train_rmse:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}, Val RMSE: {val_rmse:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Save best model checkpoint\n",
    "            if val_loss < best_val_loss:\n",
    "                print(\n",
    "                    f\"New best validation loss: {val_loss:.4f} (previous best: {best_val_loss:.4f})\"\n",
    "                )\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement = 0\n",
    "                save_checkpoint(\n",
    "                    epoch, model, optimizer, scheduler, best_val_loss, save_path\n",
    "                )\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                print(f\"No improvement, patience left: {patience - no_improvement}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if no_improvement >= patience:\n",
    "                print(f\"Early Stopping Triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training Interrupted! Saving last checkpoint...\")\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler, best_val_loss, save_path)\n",
    "\n",
    "    print(f\"Training Completed in {(time.time() - start_time):.2f} seconds\")\n",
    "\n",
    "    # Load the best model before returning\n",
    "    model, optimizer, scheduler, _, _ = load_checkpoint(\n",
    "        model, optimizer, scheduler, device, save_path\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CpGCounterAdvancedPackPadding(\n",
      "  (embedding): Embedding(6, 32, padding_idx=0)\n",
      "  (lstm): LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "new_model = CpGCounterAdvancedPackPadding(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 1/50, Train Loss: 1.2279, Train MAE: 0.7977, Train RMSE: 1.0848, Val Loss: 0.8468, Val MAE: 0.6747, Val RMSE: 0.8743\n",
      "New best validation loss: 0.8468 (previous best: inf)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 2/50, Train Loss: 1.0048, Train MAE: 0.7209, Train RMSE: 0.9796, Val Loss: 1.0637, Val MAE: 0.8400, Val RMSE: 1.0165\n",
      "No improvement, patience left: 4\n",
      "Epoch 3/50, Train Loss: 0.7115, Train MAE: 0.5995, Train RMSE: 0.8237, Val Loss: 0.4988, Val MAE: 0.5230, Val RMSE: 0.6958\n",
      "New best validation loss: 0.4988 (previous best: 0.8468)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 4/50, Train Loss: 0.4069, Train MAE: 0.4263, Train RMSE: 0.6082, Val Loss: 0.4523, Val MAE: 0.4793, Val RMSE: 0.6620\n",
      "New best validation loss: 0.4523 (previous best: 0.4988)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 5/50, Train Loss: 0.2445, Train MAE: 0.3264, Train RMSE: 0.4771, Val Loss: 0.2778, Val MAE: 0.3261, Val RMSE: 0.5070\n",
      "New best validation loss: 0.2778 (previous best: 0.4523)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 6/50, Train Loss: 0.1693, Train MAE: 0.2595, Train RMSE: 0.3863, Val Loss: 0.0517, Val MAE: 0.1355, Val RMSE: 0.2089\n",
      "New best validation loss: 0.0517 (previous best: 0.2778)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 7/50, Train Loss: 0.1046, Train MAE: 0.2033, Train RMSE: 0.2999, Val Loss: 0.4022, Val MAE: 0.5135, Val RMSE: 0.6277\n",
      "No improvement, patience left: 4\n",
      "Epoch 8/50, Train Loss: 0.1133, Train MAE: 0.2161, Train RMSE: 0.3105, Val Loss: 0.0266, Val MAE: 0.0928, Val RMSE: 0.1507\n",
      "New best validation loss: 0.0266 (previous best: 0.0517)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 9/50, Train Loss: 0.0845, Train MAE: 0.1837, Train RMSE: 0.2710, Val Loss: 0.1248, Val MAE: 0.2035, Val RMSE: 0.3270\n",
      "No improvement, patience left: 4\n",
      "Epoch 10/50, Train Loss: 0.0968, Train MAE: 0.1928, Train RMSE: 0.2775, Val Loss: 0.0259, Val MAE: 0.0979, Val RMSE: 0.1532\n",
      "New best validation loss: 0.0259 (previous best: 0.0266)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 11/50, Train Loss: 0.0638, Train MAE: 0.1617, Train RMSE: 0.2363, Val Loss: 0.0330, Val MAE: 0.1128, Val RMSE: 0.1737\n",
      "No improvement, patience left: 4\n",
      "Epoch 12/50, Train Loss: 0.0871, Train MAE: 0.1818, Train RMSE: 0.2627, Val Loss: 0.2353, Val MAE: 0.3432, Val RMSE: 0.4810\n",
      "No improvement, patience left: 3\n",
      "Epoch 13/50, Train Loss: 0.0855, Train MAE: 0.1798, Train RMSE: 0.2602, Val Loss: 0.3115, Val MAE: 0.3716, Val RMSE: 0.5510\n",
      "No improvement, patience left: 2\n",
      "Epoch 14/50, Train Loss: 0.0773, Train MAE: 0.1745, Train RMSE: 0.2516, Val Loss: 0.0116, Val MAE: 0.0559, Val RMSE: 0.0988\n",
      "New best validation loss: 0.0116 (previous best: 0.0259)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 15/50, Train Loss: 0.0638, Train MAE: 0.1562, Train RMSE: 0.2146, Val Loss: 0.0119, Val MAE: 0.0573, Val RMSE: 0.0972\n",
      "No improvement, patience left: 4\n",
      "Epoch 16/50, Train Loss: 0.0707, Train MAE: 0.1735, Train RMSE: 0.2300, Val Loss: 0.0066, Val MAE: 0.0464, Val RMSE: 0.0781\n",
      "New best validation loss: 0.0066 (previous best: 0.0116)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 17/50, Train Loss: 0.0609, Train MAE: 0.1579, Train RMSE: 0.2095, Val Loss: 0.0058, Val MAE: 0.0408, Val RMSE: 0.0731\n",
      "New best validation loss: 0.0058 (previous best: 0.0066)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 18/50, Train Loss: 0.0510, Train MAE: 0.1443, Train RMSE: 0.1988, Val Loss: 0.0076, Val MAE: 0.0554, Val RMSE: 0.0858\n",
      "No improvement, patience left: 4\n",
      "Epoch 19/50, Train Loss: 0.0780, Train MAE: 0.1781, Train RMSE: 0.2289, Val Loss: 0.0058, Val MAE: 0.0452, Val RMSE: 0.0713\n",
      "No improvement, patience left: 3\n",
      "Epoch 20/50, Train Loss: 0.0531, Train MAE: 0.1498, Train RMSE: 0.2058, Val Loss: 0.0075, Val MAE: 0.0519, Val RMSE: 0.0812\n",
      "No improvement, patience left: 2\n",
      "Epoch 21/50, Train Loss: 0.0444, Train MAE: 0.1314, Train RMSE: 0.1820, Val Loss: 0.0050, Val MAE: 0.0408, Val RMSE: 0.0682\n",
      "New best validation loss: 0.0050 (previous best: 0.0058)\n",
      "Model checkpoint saved at best_cpg_model_packpad.pth\n",
      "Epoch 22/50, Train Loss: 0.0568, Train MAE: 0.1573, Train RMSE: 0.2074, Val Loss: 0.0052, Val MAE: 0.0419, Val RMSE: 0.0686\n",
      "No improvement, patience left: 4\n",
      "Epoch 23/50, Train Loss: 0.0610, Train MAE: 0.1557, Train RMSE: 0.2061, Val Loss: 0.0062, Val MAE: 0.0443, Val RMSE: 0.0763\n",
      "No improvement, patience left: 3\n",
      "Epoch 24/50, Train Loss: 0.0624, Train MAE: 0.1672, Train RMSE: 0.2176, Val Loss: 0.0120, Val MAE: 0.0657, Val RMSE: 0.0996\n",
      "No improvement, patience left: 2\n",
      "Epoch 25/50, Train Loss: 0.0657, Train MAE: 0.1531, Train RMSE: 0.2019, Val Loss: 0.0200, Val MAE: 0.0933, Val RMSE: 0.1334\n",
      "No improvement, patience left: 1\n",
      "Epoch 26/50, Train Loss: 0.0463, Train MAE: 0.1411, Train RMSE: 0.1911, Val Loss: 0.0079, Val MAE: 0.0555, Val RMSE: 0.0847\n",
      "No improvement, patience left: 0\n",
      "Early Stopping Triggered after epoch 26\n",
      "Training Completed in 20.26 seconds\n",
      "Loaded checkpoint from best_cpg_model_packpad.pth, resuming from epoch 21 with best validation loss: 0.0050\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model_pack_padded(\n",
    "    new_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    device,\n",
    "    epochs=num_epochs,\n",
    "    patience=5,\n",
    "    save_path=\"best_cpg_model_packpad.pth\",\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cpgs_from_dna_pack_padded(\n",
    "    model_path: str,\n",
    "    dna_sequence: str,\n",
    "    dna2int: dict,\n",
    "    embedding_dim,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    device,\n",
    "    model_class=CpGCounterAdvanced,  # Ensure the correct model class is used\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict CpG count from a human DNA string.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path: Path to trained LSTM model.\n",
    "    - dna_sequence: Human-readable DNA string.\n",
    "    - dna2int: Dictionary mapping DNA bases to integer values.\n",
    "    - embedding_dim: Dimension of embedding layer.\n",
    "    - hidden_size: Size of LSTM hidden state.\n",
    "    - num_layers: Number of LSTM layers.\n",
    "    - dropout: Dropout rate.\n",
    "    - device: The device ('cpu' or 'cuda') for inference.\n",
    "    - model_class: The model class to initialize the architecture.\n",
    "\n",
    "    Returns:\n",
    "    - Predicted CpG count (rounded to 2 decimal places).\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the model checkpoint exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model checkpoint not found at {model_path}\")\n",
    "\n",
    "    # Load Model\n",
    "    vocab_size = len(dna2int)\n",
    "    model = model_class(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "\n",
    "    # Load the trained model checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)  # Move model to the correct device\n",
    "    model.eval()\n",
    "\n",
    "    # Convert DNA string to integer sequence\n",
    "    int_sequence = [\n",
    "        dna2int.get(base, 0) for base in dna_sequence\n",
    "    ]  # Map bases to integers\n",
    "    int_tensor = (\n",
    "        torch.tensor(int_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    )  # Add batch dim\n",
    "\n",
    "    # Compute sequence length (as tensor) and move to the same device\n",
    "    lengths = torch.tensor([len(int_sequence)], dtype=torch.long).to(device)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        predicted_count = (\n",
    "            model(int_tensor, lengths).squeeze().item()\n",
    "        )  # Ensure it's a scalar\n",
    "\n",
    "    return round(predicted_count, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA: NCAACGCGNAGCTCGGCNAGCTCG \n",
      "🔹 Predicted CpG Count: 4.18\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "test_dna = \"NCAACGCGNAGCTCGGCNAGCTCG\"\n",
    "# no of cpg sites in the test example is 4\n",
    "\n",
    "\n",
    "predicted_cpgs = predict_cpgs_from_dna_pack_padded(\n",
    "    \"best_cpg_model_packpad.pth\",\n",
    "    test_dna,\n",
    "    dna2int,\n",
    "    embedding_dim,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    device,\n",
    "    model_class=CpGCounterAdvancedPackPadding,\n",
    ")\n",
    "\n",
    "print(f\"DNA: {test_dna} \\n🔹 Predicted CpG Count: {predicted_cpgs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################ END ################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emlo_env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
